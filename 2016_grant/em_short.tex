
\vspace{\up}
\paragraph{EM Algorithm}

The problem with estimating the parameters ($\bm{\theta}$) of a
utility function in this setting is that there is a latent variable
vector $X$ (or rather, some of the elements of the $X$ vector are
latent, and some may be observed), which makes it difficult to compute
the likelihood of the model and maximize its parameters.  The EM
approach to solving this problem is to first choose values for
$\bm{\theta}$; then choose a new $\bm{\theta}$ that maximizes the
expected value of the log likelihood function, where the distribution
in the expectation is the probability of the latent variables (the
$X$s in our case), given the observed data and previous choice of
$\bm{\theta}$.  The maximization can be performed using gradient
ascent, and this process repeats until convergence.

To simplify the exposition of the EM algorithm, we introduce the
notation $\bm{x}_k$ to indicate the values of the subset of observed
(i.e., known) elements in an $\bm{x}$ vector, and $\bm{x}_u$ to
represent a possible assignment to the subset of unobserved values.
Using this notation, the EM algorithm, at a high level, is summarized
in Algorithm~\ref{alg:em}.
%
We have derived a dynamic programming algorithm that, coupled with
importance sampling, yields a tractable version of EM for the
generalized IRL problem.

\begin{algorithm}
\caption{EM Algorithm for GIRL}
\begin{algorithmic}
\Require{initial $\bm{\theta}^0$, and data $\bm{s}, \bm{a}, \bm{x}_k, l$}
\For{$t=0$ to $T$}
\State $\bm{\theta}_{t+1} \gets \arg \max_{\bm{\theta'}} \E_{\bm{x}_u \sim \Pr(\bm{x}_u \mid l, \bm{x}_k, \bm{s}, \bm{a}, \bm{\theta})} \left[ \log\mathcal{L}(\bm{s}, \bm{a}, \bm{\theta}' \mid l, \bm{x}) \right]$
\EndFor
\end{algorithmic}
\label{alg:em}
\end{algorithm}

