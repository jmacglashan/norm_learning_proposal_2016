
\section{Introduction}
\label{sec:intro}

Much of human social life occurs in contexts where people must
coordinate their actions with those of others.  From party planning to
flying a rocket ship to the moon, groups of people have accomplished
great things by reasoning as a team and engaging in jointly
intentional behavior~\cite{searle1995construction}.  Indeed, some have
argued that because most other animals lack the capacity to work
adaptively as cohesive unit across many domains, team reasoning may be
the hallmark of human sociality~\cite{tomasello2005understanding}.

%Can artificial agents be successfully designed to join in human collective behavior?
%What are the psychological processes and computational mechanisms underlying this phenomenon?

Artificial agents are becoming ubiquitious in everyday life.
Collaborating with an artificial agent, however, is not as easy as
collaborating with a (real) person.  On the contrary, such
collaborations sometimes require that we behave strangely so that we
can direct the artificial agent towards productive actions.  An
artificial agent capable of engaging in human-like jointly intentional
behavior 
%or team reasoning 
would be preferable.  But how can we design artificial agents to
collaborate with people in a human-like manner?

For starters, we need a model of human behavior: i.e., a way of
explaining human actions.  One possibility is to assume an individual
to be \emph{Homo economicus}, so that actions are strictly individual
choices and objectives, made without regard for the collective.
Alternatively, one might assume an individual to be \emph{Homo
  sociologicus}, in which case actions are motivated by social goals.

By now, psychologists and behavioral economists have more or less
rejected the homo-economicus model as an accurate model of human
behavior~\cite{Kahnemann, etc.}, and have proposed countless
homo-sociologicus models in its stead~\cite{ADD CITATIONS}.
%
Consistent with these findings, this proposal is founded on the
assumption that \mydef{social preferences} hold the key to successful
collaboration.
%
Social preferences are in part \mydef{distributive}, meaning they
pertain to the qualities of an outcome, be it efficient or equitable;
and they are in part \mydef{reciprocal}, meaning they pertain to the
perceived intentions of the actors.

Moreover, following Bacharach~\cite{2006}, not only do we assume that
preferences are social rather than individual, we go one step further
and assume that decision-making takes the form of ``What should
\emph{we\/} do?'' rather than ``What should \emph{I\/} do?''  In other
words, we move from the usual assumption of best-reply reasoning to
one of \mydef{team reasoning}~\cite{TEAM REASONING: Bacharach 1999}.
We call optimal decision making, when agents hold social preferences
and employ team reasoning, \mydef{socially rational} behavior.
\commenta{Michael, what do you think of this terminology? is it too bold!? (see next sentence.)}

Under the assumption that human agents are, perhaps boundedly but
nonetheless ideally, socially rational creatures, we propose to design
socially-rational artificial agents as well.  Further, we plan to use
game theory to model interactions among socially-rational agents, both
human and artificial.

Like traditional game theory, our proposed ``social'' model of game
theory is rarely predictive, because non-zero-sum games (i.e., games
that are not strictly competitive) often yield a multiplicity of
equilibria.  We solve this problem by using learning in repeated play
to disambiguate the uncertainty that each agent might otherwise have
as to what role it will play in determining the outcome of the
game~\cite{add citations of learning as eqm selection}.  As
Shakespeare said, ``one man in his time plays many parts;'' likewise,
our aim is to design versatile artificial agents, capable of learning
their part in whatever equilibrium emerges in repeated play.

We contend that the computational process by which collaborative
behavior emerges strongly depends on social rationality, coupled with
learning via repeated interaction.
%
Specifically, the computational process is thus:
1.~agents represent social preferences using a \mydef{social utility function}
tailored to the environment and the actors;%
\footnote{This utility function is described in terms of features of the environment and the actors;
as such, it is applicable to other environments and actors with similar features.}
2.~based on their estimate of this social utility function, agents use team reasoning to make plans to act;
3.~agents act, simultaneously or sequentially;
4.~agents update their estimate of their social utility function.
Steps 2 through 4 repeats so long as the interaction continues.

%this process didn't come out of thin air
Support for this process can be found in the work of
philosophers~\cite{dennett87} and psychologists~\cite{heider44} who
have long argued that people understand and predict the actions of
others in terms of their mental states (beliefs, desires, intentions,
etc.).  For example, if you see your son pour cereal into a bowl and
then walk over to the refrigerator, you might assume he believes there
is milk in the refrigerator.  If you were near the refrigerator, he
might have asked you to bring him the milk, and you would presumably
have done so willingly, as you likely get pleasure from your son---an
other---achieving his goals.  Furthermore, after several occurrences
like this one, you may instinctively bring the milk over to your son.

\commenta{INCLUDE/EXPAND IF WE TALK ABOUT LEARNING SOCIAL NORMS, which involve sanctions!}
A social utility function is an immediate generalization of the usual
notion of an individual's utility function, but it ascribes value to
the outcome as it relates to all agents in the environment.  That is,
a social utility function combines the individual utilities of all the
agents in some way.  For example, one social utility function might be
utilitarian (seeking to maximize the total agent welfare), and another
egalitarian (seeking to maximize the minimum welfare across agents).
\commenta{these are only distributive; need an example of reciprocating -- maybe punishment!}

When agents make a plan that optimizes a social utility function, they
are choosing actions that jointly optimize the behavior of the
collective.
%, as specified by the social preferences encoded in that function.  
Likewise, when
agents update their estimate of the social utility function, they do
so using the observed history of joint actions in the environment.
%%%can we guarantee convergence!!??
If this process stabilizes, the social utility function represents the
preferences of the community as a whole, and the equilibrium
implements a joint plan of action that optimizes that social utility
function.  Indeed, in our computational model, emergent collaborative
behavior and social preferences are expected to reinforce one another.

In the proposed work, we aim to demonstrate that this computational
model has legs.  Doing so requires that we further specify how step 2
and step 4 are to be carried out.  Step 2 is a \mydef{planning} step;
as such, it can be accomplished using any off-the-shelf planner for
sequential decision making environments~\cite{Bellman!!??}
\commenta{Michael, please include citations}.

Step 4 can be thought of as a \mydef{learning from demonstration}
problem~\cite{?}~\commenta{Michael, please add citations}.
In \mydef{learning from demonstration}, an agent learns how to behave
in an environment by observing an expert.  That is, an agent is
presented with a data set consisting of multiple examples of behaviors
(e.g., state-action trajectories through a Markov decision process),
and is tasked with the objective of learning a policy capable of
(closely) reproducing the data.

Two widely studied approaches to this problem include
\mydef{imitation} and \mydef{intention} learning~\cite{???}.
~\commenta{Michael, please add citations}.
Imitation learning is a supervised learning approach, in which
an agent learns a mapping from states to actions,
with the expert's behavior serving as training data.
After training, a learner need only follow its learned policy directly.

Learning intentions, in contrast, is often framed as an 
\mydef{inverse reinforcement learning} (IRL) problem~\cite{IRL???}.
\commenta{Michael, please include citations}.
Here, the agent's goal is to learn rewards that motivate the expert's
behavior, after which it applies a planning algorithm to derive a
policy that is consistent with those learned rewards.
A strength of the IRL framework is that even simple reward functions
can capture complex behavior, leading to greater generalization
capabilities (i.e., appropriate behavior in unobserved states) than
the more direct approach taken by imitation learning.

The approach we take herein is to use IRL in step 4.  Indeed, in this
proposal, we we describe preliminary experiments with one standard IRL
algorithm, thereby demonstrating the potential of our computational model.
%
But to better support our goal of human-machine collaboration, we also
propose a new model of IRL in which demonstrations are not, by
default, assumed to have been generated by an expert.  Indeed, in the
computational process we put forth, learning is online, not off, so
not all demonstrations should be interpreted as optimal behavior.

The new technology that we plan to develop as part of this proposal is
a new model of IRL, and accompanying algorithms, which allows for both
positive and negative (not necessarily binary) labels, and moreover,
which also allows for partial feedback, meaning subtrajectories can be
ascribed such labels.
\commenta{Michael, can you please insert some stuff here about why our
  new IRL technology is so awesome. I think we want to say 1. how
  human-machine collaboration cannot be accomplished without it, and
  2. it is also good for a litany of other straight IRL tasks, such as \ldots}

\commenta{might be a way to better organize this, so that we don't have to repeat ourselves.}
Once again, using an off-the-shelf IRL algorithm, we present a
preliminary set of simulation results showing how
reinforcement-learning agents
%interacting in a grid game environment 
can represent social preferences using a social utility function, and
can then learn collaborative behavior via an interactive learning
algorithm that mimics the aforementioned computational process, so
that eventually the emergent collaborative behavior and the learned
social preferences reinforce one another.  We then go one step further
and show that our approach can also learn (in an off-line, batch
fashion) from human-human trace data so that the collaborative
behavior that is learned mimics that which is expressed in the trace
data.  While these two experiments (on machine-machine, interactive
and human-human batch data) are indeed promising, it remains to extend
these techniques to human-machine interactive data.

\commenta{I don't like any of this...}
Economists use utility functions as a potential model of why people
behave the way that they do.  It is not that they believe people
necessarily possess such a thing as a utility function; it is simply
that human behavior can perhaps be described ``as if'' people
did~\cite{Savage1954}.  One strong assumption that we have employed in
our work so far is that the social preferences of the players in a
game can be represented by a \emph{single\/} social utility
function---in other words, the collective behavior of agents can be
understood ``as if'' there were one social utility function.  Under
this assumption, planning can be carried out by decentralized agents,
and coordination on a joint plan of action can still be achieved, as
the agents all observe, and learn from, the same joint history.
% -- so there exists a unique optimal joint plan !!
%In particular, they learn the social utility function from that
%history, based on which they (independently) compute a joint plan and
%(independently) carry out their corresponding role.

One important direction for future research is to understand the
extent to which we can relax this assumption and instead assume that
individual agents behave ``as if'' they had their own individual
(social) utility functions.  The PIs on this project have extensive
experience with multi-agent reinforcement-learning algorithms in games
where each agent has its own individual (non-social) utility function.
Many of these learning algorithms are fraught with difficulty,
however, because planning tends to yield multiple equilibria, making
coordination on a joint plan of action difficult, if not impossible,
to achieve in a decentralized manner.

We are proposing to get around this problem by updating the usual
model of an agent's utility function, which ordinarily depends only on
their own material benefits, with a social utility function that
incorporates their material benefits as well as a social component.
Moreover, that social component is to be learned through interaction.
Because the agents will learn the social component from a shared joint
history, it seems plausible that such interaction could once again
lead to a situation in which emergent collaborative behavior and the
learned social preferences reinforce one another.
\commenta{up through here! It's a small technical detail, not relevant for the introduction.}

The ultimate goal in developing artificial agents that act
intelligently in multi-agent scenarios is to apply them to real-world
problems.  To a limited extent, this goal has already been achieved:
artificial agents trade stocks and bid in online ad auctions.  In
these two market environments, rationality may well be an appropriate
model of behavior for the participating agents.  However,
%given the plethora of experimental results demonstrating that human
%behavior does not abide by standard definitions of
%rationality~\cite{Camerer:2003,kahnemanst82}, 
an artificial agent that plans its collaboration by assuming the
humans in its environment will act selfishly is unlikely to be
successful in its collaborations~\cite{Camerer:2003,kahnemanst82}.
Instead of individual rationality as the underlying principle guiding
human behavior, this proposal is grounded in the assumption that
humans are socially rational beings, and artificial agents should be
too!  

