\section{Introduction}
\label{sec:intro}

Much of human social life occurs in contexts where people must
coordinate their actions with those of others.  From party planning to
flying a rocket ship to the moon, groups of people have accomplished
great things by reasoning as a team and engaging in jointly
intentional behavior~\cite{searle1995construction}.  Indeed, some have
argued that, because most other animals lack the capacity to work
adaptively as cohesive unit across many domains, team reasoning may be
the hallmark of human sociality~\cite{tomasello2005understanding}.

%Can artificial agents be successfully designed to join in human collective behavior?
%What are the psychological processes and computational mechanisms underlying this phenomenon?

Artificial agents are becoming ubiquitious in everyday life.  For
these agents to collaborate effectively with humans, however, they will
need behave in human-like ways.  Unfortunately for agent designers,
%there are many ways that people decide how to behave so that their actions result in coordinated behavior. 
there is no simple behavioral rule (or set of rules) that could be
prescribed to cover all contexts, communities, and cultures.  People
perpetually learn new rules and alter their presumed roles,%
\footnote{As Shakespeare said, ``one man in his time plays many parts.''}
and so must artificial agents.

%As Shakespeare said, ``one man in his time plays many parts;'' likewise, artificial agents must be versatile, capable of learning new rules and altering their presumed roles, as necessary.

% alternatively, maybe another way of motivating the shift to agents is to say that people have a ton of implicit rules that govern our interactions and programming them all is impossible. Indeed, this set of rules is constantly being extended and varies from group to group, so learning is absolutely necessary.

% Collaborating with an artificial agent, however, is not as easy as
% collaborating with a (real) person.  On the contrary, such
% collaborations sometimes require that we behave strangely so that we
% can direct the artificial agent towards productive actions.  An
% artificial agent capable of engaging in human-like jointly intentional
% behavior 
%or team reasoning 
% would be preferable.  But how can we design artificial agents to
% collaborate with people in a human-like manner?

Most attempts to build collaborative, artificial agents begin with a
model (however flawed) of human behavior.
%We propose a decision framework with which to endow artificial agents that is compatible with human behavior.
%: i.e., a way of selecting actions that is consistent with what human beings do.
%
One possibility is to assume we are \emph{Homo economicus}---a term
from the 1800s that portrays individuals as being guided by self interest,
and taking actions without regard to their impact on the collective.
Alternatively, one might assume an individual to be \emph{Homo
  sociologicus}---a more recent term depicting individuals as pursuing
goals imprinted on them by society.
% in which case actions are motivated by social goals.

The homo-sociologicus model was not intended as a complete description
of human behavior, which is clearly self interested at times.  But,
psychologists and behavioral economists have also more or less
rejected the homo-economicus model (e.g.~\cite{kahnemanst82}),
proposing countless homo-sociologicus-leaning models in its
stead (e.g.~\cite{forsythe94,falk2003nature,fehr1999theory,fehr2006economics,kahneman86}).
%
Consistent with these findings, this proposal is founded on the
assumption that 
%self interest incorporating 
\mydef{social preferences} hold the key to successful collaborations.
%
Social preferences are in part \mydef{distributive}, meaning they
evaluate how resources are allocated, e.g., efficiently or equitably;
and they are in part \mydef{reciprocal}, meaning they evaluate the
perceived intentions of the actors in the environment.

\comment{
  %%% I AM NOT READY TO SAY THIS AT THIS STAGE...WE ARE BUILDING UP TO IT...IT IS PART OF THE COMPUTATIONAL MODEL -- somewhat orthogonal to what preferences are about, which is all we are talking about at this stage.
They are also \mydef{emergent}, in the sense that there is no \emph{a
  priori} statement of utilities that a social agent needs to follow,
necessitating a learning process to define them.
}

We plan to model interactions among such social agents, both human and
artificial, in the framework of game theory.  
%
Following Bacharach~\cite{bacharach2006beyond}, not only do we assume
that preferences are social rather than individual, we go one step
further and assume that decision making takes the form of ``What
should \emph{we\/} do?'' rather than ``What should \emph{I\/} do?''
In other words, we move from the usual assumption of best-reply
reasoning~\cite{cournot} in games to one of \mydef{team
  reasoning}~\cite{TEAM REASONING: Bacharach 1999}.

We call optimal decision making, when agents hold
%learn
social preferences and employ team reasoning, \mydef{socially
  rational} behavior.  Social rationality is a blend of the \emph{Homo
  economicus\/} and \emph{Homo sociologicus\/} concepts: agents
optimize a \mydef{social utility function} (i.e., a representation of
social preferences), which is sufficiently rich to incorporate
%learned
perceived (possibly inferred, possibly imagined) societal benefits.

We contend that the computational process by which collaborative
behavior emerges strongly depends on social rationality, coupled with
learning via repeated interaction.
%
Specifically, the computational process is thus:
1.~agents represent social preferences using a social utility function
tailored to the environment and the actors;%
\footnote{This utility function is described in terms of features of the environment and the actors;
as such, it is applicable to other environments and actors with similar features.}
2.~based on their estimate of this function, agents use team reasoning to make plans to act;
3.~agents act, simultaneously or sequentially, as is appropriate in their environment;
4.~agents update their estimates of their social utility functions.
Steps~2 through~4 repeat so long as the interaction continues.

When agents
%use team reasoning to 
make a plan that optimizes a social
utility function, they are choosing actions that jointly optimize the
behavior of the collective.
%, as specified by the social preferences encoded in that function.  
When agents update their estimate of the social utility function, they
do so using the observed history of joint actions in the environment.
%%%can we guarantee convergence!!??
If this process stabilizes, the social utility function represents the
preferences of the community as a whole, 
%including guidance of the roles of the individual members, 
and the equilibrium implements a joint plan of action that optimizes
that social utility function.  Indeed, in our computational model,
emergent collaborative behavior and social preferences are expected to
reinforce one another.

\comment{
Support for this model can be found in the work of
philosophers~\cite{dennett87} and psychologists~\cite{heider44} who
have long argued that a person understands and predicts the actions of
others in terms of their mental states (beliefs, desires, intentions,
etc.).  For example, if you see your son pour cereal into a bowl and
then walk over to the refrigerator, you might assume he believes he
can find milk there, that he wants some, and that he intends to
retrieve some.  If you were near the refrigerator, he might have asked
you to bring him the milk, and you would presumably have done so
willingly, as you likely get pleasure from your son---an
other---achieving his goals.  Furthermore, after several occurrences
like this one, you may proactively bring the milk over to your son.
\commenta{i think this provides only weak evidence for our model; can we strengthen this somehow? or should we delete it?}
}

Classical game theory assumes that utility functions are exogeneously
determined: i.e., agents come to the table with fixed
extrinsically-set utilities.
%``Social'' game theory, with its emphasis on social preferences, must
%answer the question of where social utility functions come from: i.e.,
%how does one agent know the social preferences of another?
However, there is broad agreement in the judgment and decision-making
community that preferences are \emph{constructed}, not
elicited~\cite{Payne_Bettman_Johnson_1993}, in response to available
choices.  Likewise, our computational framework does not assume that
social preferences exist extrinsically; rather, they are constructed
in response to observed behaviors.  In our conception of social
rationality, agents \emph{learn\/} social preferences through repeated
play of the game.

A social utility function is an immediate generalization of the usual
notion of an individual's utility function, but it ascribes value to
the outcome as it relates to all actors in the environment.
%
%That is, a social utility function combines the individual utilities
%of all the agents in some way.  For example, one social utility
%function might be utilitarian (seeking to maximize the total agent
%welfare), and another egalitarian (seeking to maximize the minimum
%welfare across agents).
%
As such, it can be used to capture important elements of an artificial
agent's decision making, including (1) how it values the utility of
others~\cite{littman01d}, (2) how it values the utility of intermediate goals~\cite{macglashan15b}, and (3)
how it values sanctions against others who exhibit anti-social behavior~\cite{macglashan14c}.

We further assume that social utilities can be broken down into two
components---an objective component, which is usually a direct
function of the rules of the game, and a subjective component, which
captures notions of distributivity and reciprocity.
%
Often, the rules of a game alone give rise to multiple (objective)
equilibria, so that agents face a difficult equilibrium-selection
problem (e.g.,~\cite{schelling1980strategy}) in the underlying game.
%
We contend that socially rational behavior, in which agents optimize a
learned social utility function that reflects (constructed) social
preferences, can provide a solution to this challenging, and often
elusive, problem.
%see Section~\ref{sec:interactive} 

%\commenta{this doesn't fit any more.}
%As Shakespeare said, ``one man in his time plays many parts;''
%likewise, our aim is to design versatile artificial agents, capable of learning
%their part in whatever equilibrium emerges in repeated play.

\emph{Under the assumption that human agents are, perhaps boundedly
  but nonetheless ideally, socially rational creatures, we propose to
  design and build socially rational artificial agents that learn
  through repeated play, with the aim being for such agents to
  collaborate effectively with humans.}

In this proposal, we set out to demonstrate our potential for success,
by showing that our computational model is viable.  To this end,
%using an off-the-shelf IRL algorithm,
we present a preliminary set of simulation results showing how
reinforcement-learning agents, interacting repeatedly in a grid-game
environment, can represent social preferences using a social utility
function, so that eventually their emergent collaborative behavior and
their discovered social preferences reinforce one another.  We then go
one step further and show that our approach can also learn (in an
off-line, batch fashion) from human--human trace data so that the
collaborative behavior that is learned mimics that which is expressed
in the trace data.  While these two experiments (on machine--machine,
interactive and human--human batch data) are indeed promising, it
remains to extend these techniques to human--machine interactions,
preferably in experimental worlds with embodied robots.
\commenta{HUMAN TRACE DATA!}

The ultimate goal in developing artificial agents that act
intelligently in multi-agent scenarios is to apply them to real-world
problems.  To a limited extent, this goal has already been achieved:
artificial agents trade stocks and bid in online ad auctions.  In
these two market environments, rationality may well be an appropriate
model of behavior for the participating agents.  However,
%given the plethora of experimental results demonstrating that human
%behavior does not abide by standard definitions of
%rationality~\cite{Camerer:2003,kahnemanst82}, 
an artificial agent that plans its collaboration by assuming the
humans in its environment will act selfishly and/or are driven by
extrinsic utility functions is unlikely to be successful in its
collaborations~\cite{Camerer:2003,kahnemanst82}.  Instead of
individual rationality as the underlying principle guiding human
behavior, this proposal is grounded in the assumption that humans are
socially rational beings whose preferences and behaviors emerge and
reinforce one another through repeated interactions.  For artificial
agents to successfully collaborate with humans, they should be thus,
too!

