\section{Introduction}
\label{sec:intro}

Much of human social life occurs in contexts where people must
coordinate their actions with those of others.  From party planning to
flying a rocket ship to the moon, groups of people have accomplished
great things by reasoning as a team and engaging in jointly
intentional behavior~\cite{searle1995construction}.  Indeed, some have
argued that because most other animals lack the capacity to work
adaptively as cohesive unit across many domains, team reasoning may be
the hallmark of human sociality~\cite{tomasello2005understanding}.

%Can artificial agents be successfully designed to join in human collective behavior?
%What are the psychological processes and computational mechanisms underlying this phenomenon?

Artificial agents are becoming ubiquitious in everyday life. For agents to be
able to interact with us seamlessly, they need to coordinate in human-like ways.
Unfortunately for agent designers,
%there are many ways that people decide how to behave so that their actions 
% result in coordinated behavior. 
there is no simple rule that covers all contexts,
communities and cultures. 
People perpetually learn rules and construct roles and
so must our artificial agents.
% r, alternatively, maybe another way of motivating the shift to agents is to say that people have a ton of implicit rules that govern our interactions and programming them all is impossible. Indeed, this set of rules is constantly being extended and varies from group to group, so learning is absolutely necessary.

% Collaborating with an artificial agent, however, is not as easy as
% collaborating with a (real) person.  On the contrary, such
% collaborations sometimes require that we behave strangely \commentm{example?} so that we
% can direct the artificial agent towards productive actions.  An
% artificial agent capable of engaging in human-like jointly intentional
% behavior 
%or team reasoning 
% would be preferable.  But how can we design artificial agents to
% collaborate with people in a human-like manner?

For starters, we need 
a decision framework that is compatible with 
% a model of
human behavior: i.e., a way of selecting actions that is consistent with
what human beings do. One possibility is to assume we
are \emph{Homo economicus}---a term from the 1800s that portrays
individuals as being guided by self interest and taking actions 
without regard to impact on the collective.
Alternatively, one might assume an individual to be \emph{Homo
sociologicus}---a much more recent term depicting individuals 
as pursuing the goals imprinted on them by society.
% in which case actions are motivated by social goals.

The homo-sociologicus model was not intended as
a serious attempt at describing human behavior, which is clearly self 
interested at times. But, psychologists and behavioral economists have more or less
rejected the homo-economicus model as well~\cite{Kahnemann, etc.},
% as an accurate description of human
and have proposed countless
homo-sociologicus-leaning models in its stead~\cite{ADD CITATIONS}.
%
Consistent with these findings, this proposal is founded on the
assumption that self interest incorporates \mydef{social preferences} holds the key to successful
collaboration.
%
Social preferences are in part \mydef{distributive}, meaning they
pertain to the qualities of an outcome, be it efficient or equitable;
and they are in part \mydef{reciprocal}, meaning they pertain to the
perceived intentions of the actors.

Moreover, following Bacharach~\cite{2006}, not only do we assume that
preferences are social rather than individual, we go one step further
and assume that decision-making takes the form of ``What should
\emph{we\/} do?'' rather than ``What should \emph{I\/} do?''  In other
words, we move from the usual assumption of best-reply reasoning to
one of \mydef{team reasoning}~\cite{TEAM REASONING: Bacharach 1999}.
We call optimal decision making, when agents hold social preferences
and employ team reasoning, \mydef{socially rational} behavior.
It is a blend of both homo economicus (agents optimize utility) and 
homo sociologicus (and that utility factors in perceived or imagined 
benefits of adopting coordinated behaviors) concepts.

Under the assumption that human agents are, perhaps boundedly but
nonetheless ideally, socially rational creatures, we propose to design
socially-rational artificial agents as well.  Further, we plan to use
game theory to model interactions among socially-rational agents, both
human and artificial.


"Like traditional game theory, our proposed "social" model of game theory is rarely predictive, because non-zero-sum games (i.e., games that are not strictly competitive) often yield a multiplicity of equilibria.": I'd say "

i'm not so sure about this one. it's not social rationality that is at fault here, i don't think. it might be that game theory itself is incomplete without a dynamic model. that is what i was trying to say: game theory is not generally predictive. so i do not think social rationality is the culprit here.

Classical game theory assumes that utility functions are extrinsic to the 
agents---placed up decision makers from the outside. Social rationality, with its
inclusion of intrinsic social utilities, must answer the question of where these
utilities come from.
% Social rationality is incomplete without a learning component in which players resolve uncertainty about what role..." etc.
% Like traditional game theory, our proposed ``social'' model of game
% theory is rarely predictive, because non-zero-sum games (i.e., games
% that are not strictly competitive) often yield a multiplicity of
% equilibria.
We solve this problem by using learning in repeated play
to disambiguate the uncertainty that each agent might otherwise have
as to what utilities should guide its adoption of a role in the interaction.
% it will play in determining the outcome of the
% game~\cite{add citations of learning as eqm selection}.  
As
Shakespeare said, ``one man in his time plays many parts;'' likewise,
our aim is to design versatile artificial agents, capable of learning
their part in whatever equilibrium emerges in repeated play.

We contend that the computational process by which collaborative
behavior emerges strongly depends on social rationality, coupled with
learning via repeated interaction.
%
Specifically, the computational process is thus:
1.~agents represent social preferences using a \mydef{social utility function}
tailored to the environment and the actors;%
\footnote{This utility function is described in terms of features of the environment and the actors;
as such, it is applicable to other environments and actors with similar features.}
2.~based on their estimate of this social utility function, agents use team reasoning to make plans to act;
3.~agents act, simultaneously or sequentially;
4.~agents update their estimate of their social utility function.
Steps 2 through 4 repeats so long as the interaction continues.

%this process didn't come out of thin air
Support for this process can be found in the work of
philosophers~\cite{dennett87} and psychologists~\cite{heider44} who
have long argued that people understand and predict the actions of
others in terms of their \commentm{What does "their" refer to here?} mental states (beliefs, desires, intentions,
etc.).  For example, if you see your son pour cereal into a bowl and
then walk over to the refrigerator, you might assume he believes there
is milk in the refrigerator.  If you were near the refrigerator, he
might have asked you to bring him the milk, and you would presumably
have done so willingly, as you likely get pleasure from your son---an
other---achieving his goals.  Furthermore, after several occurrences
like this one, you may instinctively bring the milk over to your son.

\commenta{INCLUDE/EXPAND IF WE TALK ABOUT LEARNING SOCIAL NORMS, which involve sanctions!}
A social utility function is an immediate generalization of the usual
notion of an individual's utility function, but it ascribes value to
the outcome as it relates to all agents in the environment.  That is,
a social utility function combines the individual utilities of all the
agents in some way.  For example, one social utility function might be
utilitarian (seeking to maximize the total agent welfare), and another
egalitarian (seeking to maximize the minimum welfare across agents).
\commenta{these are only distributive; need an example of reciprocating -- maybe punishment!}

An agent's social utility function can be used to capture important elements of its 
decision making including (1) how it values the utility of others, (2) how it value 
the utility of intermediate coordination goals, and (3) how it values sanctions against
other agents acting in anti-social ways.

When agents make a plan that optimizes a social utility function, they
are choosing actions that jointly optimize the behavior of the
collective.
%, as specified by the social preferences encoded in that function.  
Likewise, when
agents update their estimate of the social utility function, they do
so using the observed history of joint actions in the environment.
%%%can we guarantee convergence!!??
If this process stabilizes, the social utility function represents the
preferences of the community as a whole, including guidance of
the roles of the individual members, and the equilibrium
implements a joint plan of action that optimizes that social utility
function.  Indeed, in our computational model, emergent collaborative
behavior and social preferences are expected to reinforce one another.

In the proposed work, we aim to demonstrate that this computational
model has legs.  Doing so requires that we further specify how step 2
and step 4 are to be carried out.  Step 2 is a \mydef{planning} step;
as such, it can be accomplished using any off-the-shelf planner for
stochastic sequential decision making environments~\cite{bellman57,Barto95,collins95,boutilier99,kearns99b,kocsis06}.

Step 4 can be thought of as a \mydef{learning from demonstration}
problem~\cite{argall09}, which is defined by 
an agent learning how to behave
in an environment by observing an expert.  That is, an agent is
presented with a data set consisting of multiple examples of behaviors
(e.g., state-action trajectories through a Markov decision process),
and is tasked with the objective of learning a policy capable of
(closely) reproducing the data.

Two widely studied approaches to this problem include
\mydef{imitation} and \mydef{intention} learning~\cite{macglashan15b}.
Imitation learning is a supervised learning approach in which
an agent learns a mapping from states to actions,
with the expert's behavior serving as training data~\cite{pomerleau93}.
After training, a learner need only follow its learned policy directly.

Learning intentions, in contrast, is often framed as an 
\mydef{inverse reinforcement learning} (IRL) problem~\cite{ng00,babes11}.
Here, the agent's goal is to learn rewards that motivate the expert's
behavior, after which it applies a planning algorithm to derive a
policy that is consistent with those learned rewards.
A strength of the IRL framework is that even simple reward functions
can capture complex behavior, leading to greater generalization
capabilities (i.e., appropriate behavior in unobserved states) than
the more direct approach taken by imitation learning.

The approach we take herein is to use IRL in step 4.  Indeed, in this
proposal, we describe preliminary experiments with one standard IRL
algorithm, thereby demonstrating the potential of our computational model.
%
But to better support our goal of human-machine collaboration, we also
propose a new approach to IRL in which demonstrations are not, by
default, assumed to have been generated by an expert.  Indeed, in the
computational process we put forth, learning is online, not off, so
not all demonstrations should be interpreted as optimal behavior.

The new technology that we plan to develop as part of this proposal is
a new model of IRL, and accompanying algorithms, which allows for both
positive and negative (not necessarily binary) labels, and moreover,
which also allows for partial feedback, meaning subtrajectories can be
ascribed such labels.
\commenta{Michael, can you please insert some stuff here about why our
  new IRL technology is so awesome. I think we want to say 1. how
  human-machine collaboration cannot be accomplished without it, and
  2. it is also good for a litany of other straight IRL tasks, such as \ldots}

\commenta{might be a way to better organize this, so that we don't have to repeat ourselves.}
Once again, using an off-the-shelf IRL algorithm, we present a
preliminary set of simulation results showing how
reinforcement-learning agents
%interacting in a grid game environment 
can represent social preferences using a social utility function, and
can then learn collaborative behavior via an interactive learning
algorithm that mimics the aforementioned computational process, so
that eventually the emergent collaborative behavior and the learned
social preferences reinforce one another.  We then go one step further
and show that our approach can also learn (in an off-line, batch
fashion) from human-human trace data so that the collaborative
behavior that is learned mimics that which is expressed in the trace
data.  While these two experiments (on machine-machine, interactive
and human-human batch data) are indeed promising, it remains to extend
these techniques to human-machine interactive data.

\commenta{I don't like any of this...}
Economists use utility functions as a potential model of why people
behave the way that they do.  It is not that they believe people
necessarily possess such a thing as a utility function; it is simply
that human behavior can perhaps be described ``as if'' people
did~\cite{Savage1954}.  One strong assumption that we have employed in
our work so far is that the social preferences of the players in a
game can be represented by a \emph{single\/} social utility
function---in other words, the collective behavior of agents can be
understood ``as if'' there were one social utility function.  Under
this assumption, planning can be carried out by decentralized agents,
and coordination on a joint plan of action can still be achieved, as
the agents all observe, and learn from, the same joint history.
% -- so there exists a unique optimal joint plan !!
%In particular, they learn the social utility function from that
%history, based on which they (independently) compute a joint plan and
%(independently) carry out their corresponding role.

One important direction for future research is to understand the
extent to which we can relax this assumption and instead assume that
individual agents behave ``as if'' they had their own individual
(social) utility functions.  The PIs on this project have extensive
experience with multi-agent reinforcement-learning algorithms in games
where each agent has its own individual (non-social) utility function.
Many of these learning algorithms are fraught with difficulty,
however, because planning tends to yield multiple equilibria, making
coordination on a joint plan of action difficult, if not impossible,
to achieve in a decentralized manner.

We are proposing to get around this problem by updating the usual
model of an agent's utility function, which ordinarily depends only on
their own material benefits, with a social utility function that
incorporates their material benefits as well as a social component.
Moreover, that social component is to be learned through interaction.
Because the agents will learn the social component from a shared joint
history, it seems plausible that such interaction could once again
lead to a situation in which emergent collaborative behavior and the
learned social preferences reinforce one another.
\commenta{up through here! It's a small technical detail, not relevant for the introduction.}

The ultimate goal in developing artificial agents that act
intelligently in multi-agent scenarios is to apply them to real-world
problems.  To a limited extent, this goal has already been achieved:
artificial agents trade stocks and bid in online ad auctions.  In
these two market environments, rationality may well be an appropriate
model of behavior for the participating agents.  However,
%given the plethora of experimental results demonstrating that human
%behavior does not abide by standard definitions of
%rationality~\cite{Camerer:2003,kahnemanst82}, 
an artificial agent that plans its collaboration by assuming the
humans in its environment will act selfishly is unlikely to be
successful in its collaborations~\cite{Camerer:2003,kahnemanst82}.
\commentm{I also think that the social utility function is providing
guidance on equilibrium selection, at least in our experiments.}
Instead of individual rationality as the underlying principle guiding
human behavior, this proposal is grounded in the assumption that
humans are socially rational beings, and artificial agents should be
too!  

