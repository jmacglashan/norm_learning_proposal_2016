
\section{Introduction}
\label{sec:intro}

Much of human social life occurs in contexts where people must
coordinate their actions with those of others.  From party planning to
flying a rocket ship to the moon, groups of people have accomplished
great things by reasoning as a team and engaging in jointly
intentional behavior~\cite{searle1995construction}.  Indeed, some have
argued that because most other animals lack the capacity to work
adaptively as cohesive unit across many domains, team reasoning may be
the hallmark of human sociality~\cite{tomasello2005understanding}.

%Can artificial agents be successfully designed to join in human collective behavior?
%What are the psychological processes and computational mechanisms underlying this phenomenon?

Artificial agents are becoming ubiquitious in everyday life.
Collaborating with an artificial agent, however, is not as easy as
collaborating with a (real) person.  On the contrary, such
collaborations sometimes require that we behave strangely so that we
can direct the artificial agent towards productive actions.  An
artificial agent capable of engaging in human-like jointly intentional
behavior 
%or team reasoning 
would be preferable.  But how can we design artificial agents to
collaborate with people in a human-like manner?

For starters, we need a model of human behavior: i.e., a way of
explaining human actions.  One possibility is to assume an individual
to be \emph{Homo economicus}, so that actions are strictly individual
choices and objectives, made without regard for the collective.
Alternatively, one might assume an individual to be \emph{Homo
  sociologicus}, in which case actions are motivated by social goals.

By now, behavioralists have more or less rejected the homo-economicus
model as an accurate model of human behavior~\cite{Kahnemann, etc.}, and have
proposed countless homo-sociologicus models in its stead~\cite{ADD CITATIONS}.
%
Consistent with their findings, this proposal is founded on the
assumption that \mydef{social preferences} hold the key to successful
collaboration.
%
Social preferences are in part \mydef{distributive}, meaning they
pertain to the qualities of an outcome, be it efficient or equitable;
and they are in part \mydef{reciprocal}, meaning they pertain to the
perceived intentions of other actors.

Under the assumption that interacting agents hold social preferences,
we use game theory to model interactions among agents (human or
artificial).  Moreover, following Bacharach~\cite{2006}, not only do
we assume that preferences are social rather than individual, we go
one step further and assume that decision-making takes the form of
``What should \emph{we\/} do?'' rather than ``What should \emph{I\/}
do?''  In other words, we move from the usual assumption of best-reply
reasoning to one of \mydef{team reasoning}~\cite{TEAM REASONING: Bacharach 1999}.
We call optimal decision making under these assumptions \mydef{socially rational}.

Like traditional game theory, our proposed ``social'' model of game
theory is rarely predictive, because non-zero-sum games (i.e., games
that are not strictly competitive) often yield a multiplicity of
equilibria.  We solve this problem using learning in repeated play to
disambiguate the uncertainty that each agent might otherwise have as
to what role it will play in determining the outcome of the
game~\cite{add citations of learning as eqm selection}.  As
Shakespeare said, ``one man in his time plays many parts;'' likewise,
our aim is to design versatile agents capable of learning their part
in whatever equilibrium emerges in repeated play.

We contend that the computational process by which collaborative
behavior emerges depends quite heavily on social preferences coupled
with team reasoning, and on learning via repeated interaction.
%
Specifically, the computational process is thus:
1.~agents represent social preferences using a \mydef{social utility function}
tailored to the environment and the actors;%
\footnote{This utility function is described in terms of features of the environment and the actors;
as such, it is applicable to other environments and actors with similar features.}
2.~based on their estimate of this social utility function, agents make plans to act;
3.~agents act, simultaneously or sequentially;
4.~agents update their estimate of their social utility function.
Steps 2 through 4 repeats so long as the interaction continues.

%this process didn't come out of thin air
Support for this process can be found in the work of
philosophers~\cite{dennett87} and psychologists~\cite{heider44} who
have long argued that people understand and predict the actions of
others in terms of their mental states (beliefs, desires, intentions,
etc.).  For example, if you see your son pour cereal into a bowl and
then walk over to the refrigerator, you might assume he believes there
is milk in the refrigerator.  If you were near the refrigerator, he
might have asked you to bring him the milk, and you would presumably
have done so willingly, as you likely get pleasure from your son---an
other---achieving his goals.  Furthermore, after several occurrences
like this one, you may learn to bring your son the milk, instinctively.

\commenta{INCLUDE ONLY IF WE TALK ABOUT LEARNING SOCIAL NORMS, which involve sanctions!}
A social utility function is an immediate generalization of the usual
notion of an individual's utility function, but it ascribes value to
the outcome as it relates to all agents in the environment.  That is,
a social utility function combines the individual utilities of all the
agents in some way.  For example, one social utility function might be
utilitarian (seeking to maximize the total agent welfare), and another
egalitarian (seeking to maximize the minimum welfare across agents).
\commenta{these are only distributive; need an example of reciprocating -- maybe punishment!}

When agents make a plan that optimizes a social utility function, they
are choosing actions that jointly optimize the behavior of the
collective.
%, as specified by the social preferences encoded in that function.  
Likewise, when
agents update their estimate of the social utility function, they do
so using the observed history of joint actions in the environment.
%%%can we guarantee convergence!!??
If this process stabilizes, the social utility function represents the
preferences of the community as a whole, and the equilibrium
implements a joint plan of action that optimizes that social utility
function.  Indeed, in our computational model, emergent collaborative
behavior and the social preferences are expected to reinforce one
another.
%%%perhaps assuming stationary agent behavior, but what does that mean here? that the individual agents are really optimizing individual utility functions that are unchanging over time?

In this proposal, we set out to demonstrate that this computational
model has legs.  We present a preliminary set of simulation results
showing how reinforcement-learning agents 
%interacting in a grid game environment 
can represent social preferences using a social utility function, and
can then learn collaborative behavior via an interactive learning
algorithm that mimics the aforementioned computational process, so
that eventually the emergent collaborative behavior and the learned
social preferences reinforce one another.  We then go one step further
and show that our approach can also learn (in an off-line, batch
fashion) from human-human trace data so that the collaborative
behavior that is learned mimics that which is expressed in the trace
data.  While these two experiments (on machine-machine, interactive
and human-human batch data) are indeed promising, it remains to extend
these techniques to human-machine interactive data.

\commenta{I don't like any of this...}
Economists use utility functions as a potential model of why people
behave the way that they do.  It is not that they believe people
necessarily possess such a thing as a utility function; it is simply
that human behavior can perhaps be described ``as if'' people
did~\cite{Savage1954}.  One strong assumption that we have employed in
our work so far is that the social preferences of the players in a
game can be represented by a \emph{single\/} social utility
function---in other words, the collective behavior of agents can be
understood ``as if'' there were one social utility function.  Under
this assumption, planning can be carried out by decentralized agents,
and coordination on a joint plan of action can still be achieved, as
the agents all observe, and learn from, the same joint history.
% -- so there exists a unique optimal joint plan !!
%In particular, they learn the social utility function from that
%history, based on which they (independently) compute a joint plan and
%(independently) carry out their corresponding role.

One important direction for future research is to understand the
extent to which we can relax this assumption and instead assume that
individual agents behave ``as if'' they had their own individual
(social) utility functions.  The PIs on this project have extensive
experience with multi-agent reinforcement-learning algorithms in games
where each agent has its own individual (non-social) utility function.
Many of these learning algorithms are fraught with difficulty,
however, because planning tends to yield multiple equilibria, making
coordination on a joint plan of action difficult, if not impossible,
to achieve in a decentralized manner.

We are proposing to get around this problem by updating the usual
model of an agent's utility function, which ordinarily depends only on
their own material benefits, with a social utility function that
incorporates their material benefits as well as a social component.
Moreover, that social component is to be learned through interaction.
Because the agents will learn the social component from a shared joint
history, it seems plausible that such interaction could once again
lead to a situation in which emergent collaborative behavior and the
learned social preferences reinforce one another.
\commenta{up through here! It's a small technical detail, not relevant for the introduction.}

\commenta{Insert stuff here about why our new IRL technology is so awesome, and
how human-machine collaboration cannot be accomplished without it.}

The ultimate goal in developing artificial agents that act
intelligently in multi-agent scenarios is to apply them to real-world
problems.  To a limited extent, this goal has already been achieved:
artificial agents trade stocks and bid in online ad auctions.  In
these two market environments, rationality may well be an appropriate
model of behavior for the participating agents.  However,
%given the plethora of experimental results demonstrating that human
%behavior does not abide by standard definitions of
%rationality~\cite{Camerer:2003,kahnemanst82}, 
an artificial agent that plans its collaboration by assuming the
humans in its environment will act selfishly is unlikely to be
successful in its collaborations~\cite{Camerer:2003,kahnemanst82}.
Instead of individual rationality as the underlying principle guiding
human behavior, this proposal is grounded in the assumption that
humans are socially rational.  \emph{We propose an interactive
  approach to simultaneously learning collaborative behavior and
  social preferences that reinforce one another.}

