\section{Introduction}
\label{sec:intro}

Much of human social life occurs in contexts where people must
coordinate their actions with those of others.  From party planning to
flying a rocket ship to the moon, groups of people have accomplished
great things by reasoning as a team and engaging in jointly
intentional behavior~\cite{searle1995construction}.  Indeed, some have
argued that because most other animals lack the capacity to work
adaptively as cohesive unit across many domains, team reasoning may be
the hallmark of human sociality~\cite{tomasello2005understanding}.

%Can artificial agents be successfully designed to join in human collective behavior?
%What are the psychological processes and computational mechanisms underlying this phenomenon?

Artificial agents are becoming ubiquitious in everyday life.  For
these agents to collaborate effectively with humans, however, they will
need behave in human-like ways.  Unfortunately for agent designers,
%there are many ways that people decide how to behave so that their actions result in coordinated behavior. 
there is no simple behavioral rule (or set of rules) that could be
prescribed to cover all contexts, communities, and cultures.  People
perpetually learn new rules and alter their presumed roles,%
\footnote{As Shakespeare said, ``one man in his time plays many parts.''}
and so must artificial agents.

%As Shakespeare said, ``one man in his time plays many parts;'' likewise, artificial agents must be versatile, capable of learning new rules and altering their presumed roles, as necessary.

% alternatively, maybe another way of motivating the shift to agents is to say that people have a ton of implicit rules that govern our interactions and programming them all is impossible. Indeed, this set of rules is constantly being extended and varies from group to group, so learning is absolutely necessary.

% Collaborating with an artificial agent, however, is not as easy as
% collaborating with a (real) person.  On the contrary, such
% collaborations sometimes require that we behave strangely so that we
% can direct the artificial agent towards productive actions.  An
% artificial agent capable of engaging in human-like jointly intentional
% behavior 
%or team reasoning 
% would be preferable.  But how can we design artificial agents to
% collaborate with people in a human-like manner?

Most attempts to build collaborative, artificial agents begin with a
model (however flawed) of human behavior.
%We propose a decision framework with which to endow artificial agents that is compatible with human behavior.
%: i.e., a way of selecting actions that is consistent with what human beings do.
%
One possibility is to assume we are \emph{Homo economicus}---a term
from the 1800s that portrays individuals as being guided by self interest,
and taking actions without regard to their impact on the collective.
Alternatively, one might assume an individual to be \emph{Homo
  sociologicus}---a more recent term depicting individuals as pursuing
goals imprinted on them by society.
% in which case actions are motivated by social goals.

The homo-sociologicus model was not intended as a complete description
of human behavior, which is clearly self interested at times.  But,
psychologists and behavioral economists have also more or less
rejected the homo-economicus model~\cite{Kahnemann, etc.}, proposing
countless homo-sociologicus-leaning models in its stead~\cite{ADD
  CITATIONS}.
%
Consistent with these findings, this proposal is founded on the
assumption that 
%self interest incorporating 
\mydef{social preferences} hold the key to successful collaborations.
%
Social preferences are in part \mydef{distributive}, meaning they
evaluate how resources are allocated, e.g., efficiently or equitably;
and they are in part \mydef{reciprocal}, meaning they evaluate the
perceived intentions of the actors in the environment.

We plan to model interactions among ``social'' agents, both human and
artificial, in the framework of game theory.  
%
Following Bacharach~\cite{2006}, not only do we assume that
preferences are social rather than individual, we go one step further
and assume that decision-making takes the form of ``What should
\emph{we\/} do?'' rather than ``What should \emph{I\/} do?''  In other
words, we move from the usual assumption of best-reply
reasoning~\cite{Cournot} in games to one of \mydef{team
  reasoning}~\cite{TEAM REASONING: Bacharach 1999}.

We call optimal decision making, when agents hold
%learn
social preferences and employ team reasoning, \mydef{socially
  rational} behavior.  Social rationality is a blend of the \emph{Homo
  economicus\/} and \emph{Homo sociologicus\/} concepts: agents
optimize a \mydef{social utility function}, which incorporates
%learned 
perceived or imagined societal benefits.

% Like traditional game theory, our proposed ``social'' model of game
% theory is rarely predictive, because non-zero-sum games (i.e., games
% that are not strictly competitive) often yield a multiplicity of
% equilibria.  We solve this problem by using learning in repeated play
% to disambiguate the uncertainty that each agent might otherwise have
% as to what role it will play in determining the outcome of the
% game~\cite{add citations of learning as eqm selection}.

% Social rationality is incomplete without a learning component in which players resolve uncertainty about what role..." etc.

Classical game theory assumes that utility functions are exogeneously
determined: i.e., agents come to the table with fixed
extrinsically-set utilities.  ``Social'' game theory, with its
emphasis on \mydef{other-regarding preferences},\commenta{Michael,
  please insert references} must answer the question of where social
utility functions come from: i.e., how does one agent know the
preferences of another?  We propose to solve this problem by using
learning in repeated play to disambiguate the uncertainty that each
agent might otherwise have as to what utilities should guide its
behavior.
% it will play in determining the outcome of the
% game~\cite{add citations of learning as eqm selection}.  

We contend that the computational process by which collaborative
behavior emerges strongly depends on social rationality, coupled with
learning via repeated interaction.
%
Specifically, the computational process is thus:
1.~agents represent social preferences using a social utility function
tailored to the environment and the actors;%
\footnote{This utility function is described in terms of features of the environment and the actors;
as such, it is applicable to other environments and actors with similar features.}
2.~based on their estimate of this social utility function, agents use team reasoning to make plans to act;
3.~agents act, simultaneously or sequentially;
4.~agents update their estimates of their social utility functions.
Steps 2 through 4 repeats so long as the interaction continues.

When agents
%use team reasoning to 
make a plan that optimizes a social
utility function, they are choosing actions that jointly optimize the
behavior of the collective.
%, as specified by the social preferences encoded in that function.  
When agents update their estimate of the social utility function, they
do so using the observed history of joint actions in the environment.
%%%can we guarantee convergence!!??
If this process stabilizes, the social utility function represents the
preferences of the community as a whole, 
%including guidance of the roles of the individual members, 
and the equilibrium implements a joint plan of action that optimizes
that social utility function.  Indeed, in our computational model,
emergent collaborative behavior and social preferences are expected to
reinforce one another.

\commenta{Can we claim that this idea of learning social utilities
simultaneously solves the problem of inferring others' utility functions,
and of eqm selection!?}

%this process didn't come out of thin air
Support for this model can be found in the work of
philosophers~\cite{dennett87} and psychologists~\cite{heider44} who
have long argued that people understand and predict the actions of
others in terms of their \commentm{What does ``their'' refer to here?}\commenta{i think this is made clear in the next sentence. if you agree, please delete these comments.} mental states (beliefs, desires, intentions,
etc.).  For example, if you see your son pour cereal into a bowl and
then walk over to the refrigerator, you might assume he believes he can find milk there,
that he wants some, and that he intends to retrieve some.  If you were near the refrigerator, he
might have asked you to bring him the milk, and you would presumably
have done so willingly, as you likely get pleasure from your son---an
other---achieving his goals.  Furthermore, after several occurrences
like this one, you may instinctively bring the milk over to your son.

Under the assumption that human agents are, perhaps boundedly but
nonetheless ideally, socially rational creatures, we propose to design
and build socially-rational artificial agents, with the aim being that
such agents will collaborate effectively with humans.  Doing so
requires that we further specify how steps 2 and 4 of the
aforementioned computational process are to be implemented.  Step 2 is
a \mydef{planning} step; as such, it can be accomplished using any
off-the-shelf planner for stochastic sequential decision making
environments~\cite{Barto95,bellman57,boutilier99,collins95,kearns99b,kocsis06}.

Step 4 can be thought of as a \mydef{learning from demonstration}
problem~\cite{argall09}, which is defined by an agent learning how to
behave in an environment by observing an expert.  That is, an agent is
presented with a data set consisting of multiple examples of behaviors
(e.g., state-action trajectories through a Markov decision process),
and is tasked with the objective of learning a policy capable of
(closely) reproducing the data.

Two widely studied approaches to this problem include
\mydef{imitation} and \mydef{intention} learning.  Imitation learning
is a supervised learning approach in which an agent learns a mapping
from states to actions, with the expert's behavior serving as training
data~\cite{pomerleau93}.  After training, a learner follows its
learned policy directly.

Learning intentions~\cite{macglashan15b}, in contrast, is often framed
as an \mydef{inverse reinforcement learning} (IRL)
problem~\cite{ng00,babes11}.  Here, the agent's goal is to learn
rewards that motivate the expert's behavior, after which it applies a
planning algorithm to derive a policy that is consistent with those
learned rewards.  A strength of intention learning is that even simple
reward functions can capture complex behavior, leading to greater
generalization capabilities (i.e., appropriate behavior in unobserved
states) than the more direct approach taken by imitation learning.

The approach we take herein is to use IRL in step 4.  Preliminary
experiments with one standard IRL algorithm, described herein,
demonstrate the potential of our computational model.
%
But to better support our goal of human-machine collaboration, we also
propose a new approach to IRL in which demonstrations are not, by
default, assumed to have been generated by an expert.  Indeed, in the
computational process we put forth, learning is online, not off, so
not all demonstrations should be interpreted as optimal behavior.

The new technology that we plan to develop as part of this proposal is
a new model of IRL, and accompanying algorithms, which allows for both
positive and negative (not necessarily binary) labels, and moreover,
which also allows for partial feedback, meaning subtrajectories can be
ascribed such labels.
\commenta{Michael, can you please insert some stuff here about why our
  new IRL technology is so awesome. I think we want to say 1. how
  human-machine collaboration cannot be accomplished without it, and
  2. it is also good for a litany of other straight IRL tasks, such as \ldots}

\commenta{might be a way to better organize this, so that we don't have to repeat ourselves.}
Once again, using an off-the-shelf IRL algorithm, we present a
preliminary set of simulation results showing how
reinforcement-learning agents
%interacting in a grid game environment 
can represent social preferences using a social utility function, and
can then learn collaborative behavior via an interactive learning
algorithm within the aforementioned computational framework, so that
eventually the emergent collaborative behavior and the learned social
preferences reinforce one another.  We then go one step further and
show that our approach can also learn (in an off-line, batch fashion)
from human-human trace data so that the collaborative behavior that is
learned mimics that which is expressed in the trace data.  While these
two experiments (on machine-machine, interactive and human-human batch
data) are indeed promising, it remains to extend these techniques to
human-machine interactions.

The ultimate goal in developing artificial agents that act
intelligently in multi-agent scenarios is to apply them to real-world
problems.  To a limited extent, this goal has already been achieved:
artificial agents trade stocks and bid in online ad auctions.  In
these two market environments, rationality may well be an appropriate
model of behavior for the participating agents.  However,
%given the plethora of experimental results demonstrating that human
%behavior does not abide by standard definitions of
%rationality~\cite{Camerer:2003,kahnemanst82}, 
an artificial agent that plans its collaboration by assuming the
humans in its environment will act selfishly is unlikely to be
successful in its collaborations~\cite{Camerer:2003,kahnemanst82}.
\commentm{I also think that the social utility function is providing
guidance on equilibrium selection, at least in our experiments.}
\commenta{???}
Instead of individual rationality as the underlying principle guiding
human behavior, this proposal is grounded in the assumption that
humans are socially rational beings, and artificial agents should be
too!  


\commenta{this doesn't fit any more.}
As Shakespeare said, ``one man in his time plays many parts;''
likewise, our aim is to design versatile artificial agents, capable of learning
their part in whatever equilibrium emerges in repeated play.
\commenta{delete?}

\commenta{where does this go?}
A social utility function is an immediate generalization of the usual
notion of an individual's utility function, but it ascribes value to
the outcome as it relates to all actors in the environment.
%
%That is, a social utility function combines the individual utilities
%of all the agents in some way.  For example, one social utility
%function might be utilitarian (seeking to maximize the total agent
%welfare), and another egalitarian (seeking to maximize the minimum
%welfare across agents).
%
As such, it can be used to capture important elements of an artificial
agent's decision making, including (1) how it values the utility of
others, (2) how it values the utility of intermediate goals, and (3)
how it values sanctions against others who exhibit anti-social behavior.
\commenta{Michael, if you have references to include with each of these bullets, i think it would strengthen the claims}
