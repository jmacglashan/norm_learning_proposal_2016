
\section{Evaluation}

Before scaling to embodied robots,\commenta{Michael, are we going to do this? if not, tone down to whatever our real-world application plans are}
we will evaluate our approach to collaborative learning using an
experimental platform that simulates interactions between humans and
artificial agents.  For example, we will compare how often and how
quickly humans are able to arrive at coordinated plans among
themselves, versus with an artificial agent.  To test the
expressiveness of the space of learnable solutions afforded by our
approach, we will also measure tendencies among humans to converge to
plans with certain characteristics, and whether those characteristics
vary when there are artificial agents in the mix.  We will also assess
the quality of the plans under these varying conditions.  Finally,
subjective evaluations from the human participants will be used to
assess whether interacting with agents feels natural.

For the batch learning algorithm, we will develop new quantitative
metrics to assess behavioral similarity so that trajectories produced
by our batch learners can be quantitatively compared to the trace data
produced by human participants.  Additionally, we plan to design a
sort of ``Turing test'' for our problem domain, whereby we will have
two humans interact with each other for a number of rounds and then,
without notification, we will fork those interactions into two, where
each human is now playing with an artificial agent that trained on the
history of interactions between the two humans.  At the end of all
interactions, we will ask the user whether they noticed anything
strange partway through the interactions, such as a change of partner.
Their subjective response to whether they thought their partner
switched at any point will be compared to their subjective responses
in other treatments, such as: (1)~no switching; (2)~swapping their
partner for another human; (3)~swapping their partner for another
human who watched the previous interactions; (4)~swapping their
partner for an artificial agent that did not perform any learning.

Finally, we are particularly interested in designing flexible learning
algorithms that generalize at least as well as humans do to new state
spaces.  To this end, we will have participants play a series of
related grid games, and then using evaluations similar to those listed
above, we will test whether our learning agents can generalize to new
games as well as other humans can.  Likewise,\commenta{Michael, do we want to say something about generalizing to other players? maybe humans are better at this than our agents will be?}

\commenta{Michael, how are we going to evaluate our approach in our proposed ``real-world'' (i.e., non-grid game) applications?}
