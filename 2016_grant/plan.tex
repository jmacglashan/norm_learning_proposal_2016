
\section{Plan}
The three-year timeline for our proposed project is as follows:

\begin{itemize}
\item {\bf Year 1}: Complete the development of an experimental
  testbed that pairs humans with an artificial agent.  Demonstrate
  that the agent can successfully learn collaborative behavior from
  batches of demonstrations (offline) including generalizations
  between related game environments.

\item {\bf Year 2}: Extend the learning algorithms to support
  real-time adaptation, so that cooperative behavior can emerge during
  a human--machine interaction even if the test environments vary. A
  key algorithmic element will be to determine whether an interaction
  is proceeding cooperatively, or if cooperation has broken down and
  players should revert to more traditional self-interested behavior. 


\item {\bf Year 3}: Extend and evaluate our approach in real-world
  applications, specifically to the ``go fetch'' scenario described in
  the Broader Impacts section. Algorithmically, we will focus on ways
  an agent can actively teach its partner to adopt mutually beneficial
  behavior. 

\end{itemize}

From our preliminary investigations, it has become clear that metrics
play an essential role in these learning scenarios.  Evaluation
metrics that capture trajectory similarity across diverse games are
key for assessing performance and also for driving learning algorithms
to more beneficial behavior. 

We will also develop ways for people to also assess how natural it is
to interact with our artificial agents. For example, we will
investigate whether having two humans interact for awhile, and then,
without notifying them, swapping their partner with an artificial
agent trained from the interaction history results in a noticeable
change in behavior. Participants would be asked if they noticed a
change, and behavior would be compared between three cases: no swap,
swapping in a trained agent, and swapping in an untrained agent.

Throughout the project, we will investigate how to generalize social
utility functions across environments. To facilitate generalization
between state spaces, we will start by examining standard features and
representations used in existing reinforcement-learning and IRL
research (e.g., linear models, neural networks, etc.) and build on
them, as necessary. 
% ML: Note that we could propose ``deep IRL''... since MLIRL is a
% gradient method, it should be easy to incorporate it into standard
% deep learning packages to learn very complex social utilities.
We will evaluate generalization by first having users learn in one
grid. Afterwards, they will be exposed to a new grid where a the
previous behavior is no longer directly applicable (e.g., because a
wall was added in the path that one agent would normally take), but a
more ``general'' behavior can be transfered (e.g., I move above the
other agent). 

% The algorithms described in this proposal are designed for learning
% among a set of agents interacting pairwise. We will extend our work to
% scenarios where agents play different games, with different subsets of
% agents drawn from the same population. We expect that a group of
% people can converge on a norm in a setting like this, and so will
% extend our algorithms as necessary to support this robust form of
% norm-learning. To evaluate our extensions, we will compare norm
% formation in purely human groups to hybrid human-machine groups.
% %One possible direction is to have the agent learn multiple norms using reward function clustering similar to that in multiple intention IRL\cite{babes11}, and then homogenize the clusters as the populations converges. 
% %To evaluate adaptivity in populations, we will compare the results of purely human populations with human-agent populations.

% In year three we will test our approach in a real-world domain:
% robot-human collaboration. Our simulated worlds suggest that our
% learning agents could collaborate better with people than traditional
% approaches, but the capabilities of robots may inhibit the
% effectiveness of our approach (e.g., robots might move slower than
% people expect). We will evaluate our algorithm in real-world scenarios
% by comparing its performance at controlling a robot to when another
% human controls the robot via teleoperation. We expect this to inspire
% novel problems tuned to real-world problems. For example, computation
% time may be a limiting factor to the success of our machine agents,
% and if so, we will develop more efficient implementations.

