
\subsection{Collaborative Learning Algorithms}
\label{sec:learning}

Given our representation of social preferences by a social reward
function that is parameterized by a family of bias functions, our
approach to collaborative learning is simply to optimize those
parameters to best match observed behavior.  We consider two forms of
collaborative learning.  The first is \mydef{batch} learning, in which
an agent learns offline from batches of demonstrations of other agents
playing collaboratively.  The second is \mydef{interactive} learning
in which agents learn the social preferences of the collective via
repeated interactions.
%
At the core of both algorithms is team reasoning.

%We consider learning norms under two different conditions. First, when
%an agent observes example behaviors of other agents conforming to a
%norm, and also when an agent must interact with a set of unknown
%agents and develop new norms that facilitate coordination. We refer to
%the first learning situation as {\em batch} norm learning, since the
%agent will receive batches of demonstrations and can perform norm
%learning offline from them; we refer to the latter situation as {\em
%  interactive} norm learning, since the agent must learn norms with
%other agents while interacting with them.%

%To perform both batch and interactive norm learning, we take an
%approach in which the agent reasons about the multi-agent interaction
%as a joint task to solve and then learns biases for behavior in the
%joint task that when coupled with the overall joint task goal, results
%in norm-following behavior. To formalize and solve this learning
%problem, we build from Markov decision process and stochastic games
%formalisms, and inverse reinforcement learning literature. We first
%describe relevant background material on these topics and then
%describe our norm-learning algorithms.

