
\vspace{\up}
\paragraph{Preliminary Experiments}

We ran some (very) preliminary experiments to demonstrate the
potential of an EM algorithm we developed for the GIRL framework.%
%\footnote{A description of the algorithm was omitted because of space constraints.}
\footnote{Because the problem domain in these preliminary experiments
was small (a 5 $\times$ 5 grid), we did not use importance sampling.}

The GIRL agent is operating in a grid world, with puddles of multiple
colors: red, blue, green, yellow and pink. The agent can take north,
south, east and west actions.  If there is a wall in the way, the
agent does not move at all.
%For example if the agent tries to move east and it is next to the eastern wall of the grid, it doesn't move at all.  
In these experiments, we gave the GIRL agent two very short
trajectories, and a corresponding pair of positive and negative
labels.  We will show that varying those labels leads the agent to
infer different reward functions, and consequently, different
behaviors.

In both trajectories, the agent begins in the middle cell of the
bottom row of the grid.  The first trajectory has the agent move two
steps to the west, through the blue puddle, to reach the red puddle.
The second trajectory has the agent move a single step west, into the
blue puddle.

%These trajectories are shown in the first set of Figures~\ref{fig:trajectories}.

In the first experiment, we label both trajectories $+1$; in the
second experiment, we label the first trajectory $+1$, and the second
$-1$; and in the third experiment, we reverse these labels.  The
learned policies (along with corresponding, but illegible, rewards)
are shown in Figures~\ref{fig:test1},~\ref{fig:test2},
and~\ref{fig:test3}, respectively.

In experiment 1, we signal to the agent that both observed behaviors
are good.  Consequently, the agent learns to value the red puddle more
than any other cell in the grid, and is intent on reaching it via a
shortest path from everywhere else.  In particular, the agent doesn't
learn to linger on the blue puddle, because in one of the trajectories
walking over the blue to reach the red was labelled good.

In experiment 2, we again teach the agent that reaching the red puddle
is good, but this time we also signal that walking into the blue
puddle is bad.  The agents again learns to get to the red puddle as
quickly as possible, but this time it avoids all blue puddles.
possible.
%As in experiment 1, the red puddle has the highest value in the grid.

In experiment 3, we teach the agent that the red puddle is to be
avoided, and getting to the blue puddle is good.  This time, the agent
prefers the blue puddles to the red, and it especially values the ones
where it can remain in the blue cells indefinitely.

\comment{
In the fourth experiment we teach the agent that both red and blue
puddles are bad. Hence, we label both trajectories $-1$.  The output
policy in Figure~\ref{fig:test4} shows that the agent avoids all the
puddles and tries to stay in the white spaces on the grid.
}

Thus, we have demonstrated that agents can learn utility functions
within the GIRL framework using both positive and negative labels.  We
expect this capability to prove essential for humans hoping to teach
agents to collaborate.  Beyond what is possible now, with GIRL
algorithms, we, the humans, will be able to critique agents during our
interactions with them, by ascribing to their behaviors (entire
trajectories and subtrajectories) both positive and negative feedback,
based on which the agents will infer social reward functions, which we
expect to generalize well within the learning domain and across
learning environments.

