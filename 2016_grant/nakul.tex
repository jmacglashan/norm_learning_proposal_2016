
\vspace{\up}
\paragraph{Preliminary Experiments}

We ran some (very) preliminary experiments to demonstrate the
potential of our EM approach in the GIRL framework.%
%\footnote{A description of the algorithm was omitted because of space constraints.}
\footnote{Because the problem domain in these experiments
was small (a 5 $\times$ 5 grid), we did not need importance sampling.}
%
The agent is operating in a grid world (see
Figure~\ref{fig:puddle-grid}), with puddles of multiple colors: red,
blue, green, yellow and pink.  The agent (gray circle) can take north, south, east
and west actions.
%If there is a wall in the way, the agent does not move at all.
%For example if the agent tries to move east and it is next to the eastern wall of the grid, it doesn't move at all.  
In these experiments, we gave the agent two very short
trajectories, and a corresponding pair of positive and negative
labels.  We show that varying these labels leads the agent to
infer different utility functions, and correspondingly different
behaviors.

In both trajectories, the agent begins in the middle cell of the
bottom row of the grid (as shown).  The first trajectory has the agent
move two steps to the west, through the blue puddle, and terminating
at the red puddle.  The second trajectory has the agent move a single
step west, terminating at the blue puddle.
%
%These trajectories are shown in the first set of Figures~\ref{fig:trajectories}.
%
We compare three different labelings for these trajectories (all three
labelings that include at least one positive label): two positive
($+1$ for each), positive \& negative ($+1$ for first, $-1$ for
second), and negative \& positive ($-1$ for first, $+1$ for
second). The learned policies, along with shading that reflects
inferred utility values (blue is high; red is low; and shades of
purple, in between)
%(along with corresponding, but illegible, rewards)
are shown in Figures~\ref{fig:test1},~\ref{fig:test2},
and~\ref{fig:test3}, respectively.

The two positive labels signal to the agent that both observed
behaviors are good.  Consequently, the agent infers that the red
puddle is worth more than any other cell in the grid and seeks
shortest paths for reaching it.  Note that the agent does not learn to
linger on the blue puddle---even though one trajectory ended
there---because both trajectories are drawn toward the red puddle in
spite of all options for approaching blue puddles elsewhere in the
grid.
% zzz is that why?

Given positive \& negative labels, the agent again infers that
reaching the red puddle is good and the blue puddle is not
good. In contrast to the previous case, however, it infers that all
other cells are also good. This example illustrates the power of
negative examples. By asserting that the path terminating at the blue
puddle is bad, the algorithm knows that one of the steps on this path
is not the correct step for an optimal agent to take. As such, the
correct path for reaching the red puddle must be to go around the blue
puddle, which can be explained by increased utility in the white
squares.
% zzz these colors are almost maximally confusing. red turns blue and
% zzz blue turns red. yellow and green would have been preferable.
% zzz also, it would have been more interesting to have a negative
% label on a two-step path as the learner has to figure out which step
% was bad.
%As in experiment 1, the red puddle has the highest value in the grid.

The agent sees negative \& positive labels as a signal that the red
puddle is to be avoided and getting to the blue puddle is good.  This
time, the agent prefers the blue puddles to the red, and it finds a
policy that lets it reach---and remain in---blue puddles.
%\commentm{these grids seem like overkill for the point we're able to make. Ah, well.}
%\commenta{agreed, totally!}

\comment{
In the fourth experiment we teach the agent that both red and blue
puddles are bad. Hence, we label both trajectories $-1$.  The output
policy in Figure~\ref{fig:test4} shows that the agent avoids all the
puddles and tries to stay in the white spaces on the grid.
}

These experiments demonstrate an approach to GIRL, in which agents
learn utility functions from both positive and negative examples.
% We expect this capability to prove essential for humans hoping to teach agents to collaborate.  
We plan to further hone this and related approaches, so that we can
eventually bring them to bear in the context of interactive learning
of social utility functions, thereby enabling artificial agents to
learn from rich human evaluative feedback.

Beyond what is possible now, with generalized IRL algorithms will allow us, the humans,
to critique agents during our interactions with them, 
ascribing to their behaviors (entire trajectories and subtrajectories)
both positive and negative feedback. Based on this feedback, the agents will
infer social reward functions, which we expect to generalize well
within the learning domain and across learning environments.
