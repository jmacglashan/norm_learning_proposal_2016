
\subsection{Mathematical Models}
\label{sec:models}

As grid games are examples of stochastic
games~\cite{Shapley53,non-zero-sum}, we use this game-theoretic
framework to model agent interactions.
%
Like econometricians~\cite{}, we assume a structural form for utility
functions; and because we are interested in building socially rational
artificial agents, we endow our agents with a (potentially) social
utility function.
%
We then develop a batch learning algorithm by which an artificial
agent can, by learning from demonstrations, recover a social utility
function that supports exactly one of many equilibria (assuming the
demonstrations exhibit exactly one of many equilibria).
%
We also develop an interactive learning algorithm by which two agents
can simultaneously learn a social utility function, and converge
%(very quickly)
to one among a plethora of equilibria, thereby endogenously solving
the equilibrium selection process.

\comment{
Both of these algorithms rely on IRL, and in our experiments thus far
they invoke only standard IRL (e.g.,~\cite{MLIRL, BIRL}).
As standard IRL cannot accommodate negative feedback,
\commenta{BLAH BLAH BLAH}
}

\input{mdp.tex}
\input{irl.tex}
\input{stochastic.tex}
\input{rewards.tex}

\comment{
WHY INTENTION LEARNING BEATS SUPERVISED METHODS!

Recall our working definition of a social norm as a behavioral
instruction that members of the community expect one another to
follow.  This definition suggests that norms could potentially be
learned in a supervised fashion simply by learning a function that
maps states to joint actions directly, rather than capturing norms in
a social reward function that motivates joint actions.
%\commenta{do you mean just hard code a norm in the bias functions? 
%does something like this next sentence capture what you mean?}
%\commenta{This definition suggests that norms could be directly encoded in the bias functions of a social reward function as bonuses associated with specific joint actions.}
However, as an environment and a target policy become more complex,
directly learning the policy becomes more challenging, and
generalizing from it likely less successful.  As with standard IRL,
the advantage of learning a social reward function instead is that
simple reward functions can often induce complex
behaviors \emph{across states}.  For example, consider a seemingly
straightforward norm that two agents approaching one another each stay
to their right.  If the action space is over low-level controls (e.g.,
rotations and small movements) and the policy is context dependent
(e.g., navigating obstacles to get to the right side), implementing
all the joint action rules necessary to make the agents pass on the
right could be difficult to specify (and hence, learn).  However, a
social reward function can simply define a bonus for when the agents
move past each other on the right and the planning algorithm does the
rest.
}

To facilitate social reward functions that generalize, the bias
function family must operate on a set of useful features.  As part of
this project, we plan to investigate whether existing state-of-the-art
feature selection methods for
RL~\cite{diuk2009adaptive,kolter2009regularization,li2009reinforcement,parr2008analysis}
work well in our setting, and then work towards developing new methods
to the extent necessary.

%A viable alternative might be to design the bias functions such that
%agents can receive a bonus as they move past each other on the right.
%The intent is that motivating the desired behavior in this way would
%result in greater generalization power.  To facilitate this kind of
%generalization, the bias function family must operate on a set of
%useful features.  As part of this project, we plan to investigate
%whether existing state-of-the-art feature selection methods for
%RL~\cite{diuk2009adaptive,kolter2009regularization,li2009reinforcement,parr2008analysis}
%work well in our setting, and then to work towards developing new
%methods to the extent necessary.

%\jmnote{I made a bit of a bold claim here saying that we would investigate feature selection. I say that because this might indeed be an interesting space that benefits from new kinds of features, but if we don't want to commit to this, we can just say its outside the scope of our work.}

\comment{
We believe that the choice to represent norms by a single social
reward function is grounded in sound psychology because\commenta{JOE!!??}.
%
Furthermore, this assumption is essential for our approach to be
tractable, because IRL in MDPs is becoming increasingly easier, while
IRL in stochastic games remains enormously challenging.
}

