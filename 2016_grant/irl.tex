
\paragraph{Inverse reinforcement learning}

Although there are multiple IRL formalizations and approaches, they
all take as input an MDP together with a \mydef{family} of a reward
functions $R_\Theta$ that is defined by some parameter space $\Theta$,
and a dataset $D$ of trajectories (where a trajectory is a finite
sequence of state-action pairs: $\langle (s_1, a_1), ..., (s_n, a_n)
\rangle$). The algorithms then output a specific parameterized reward
function $R_\theta \in R_\Theta$, which induces a policy that is
consistent with the input trajectories.
%
Different IRL algorithms frame the objective function for policy
consistency differently. One common approach is to treat the
search problem as a probabilistic inference
problem~\cite{babes11,lopes2009active,ramachandran2007bayesian,ziebart2008maximum}. 

For example, in the maximum-likelihood setting~\cite{babes11}, the
goal is to find a reward function parameterization that maximizes a
likelihood function:
%
\begin{equation}
\label{eq:mlirl}
\theta \in \arg\max_{\theta} L(D \mid R_{\theta}) = \prod_{t \in D} \prod_i^{|t|} \pi_{\theta}(s_i, a_i),
\end{equation}

where $\pi_{\theta}(s, a)$ is a stochastic policy defining the
probability of taking action $a$ in state $s$ when the parameterized
reward function to be maximized is $R_{\theta}$. Typically, the
Boltzmann (softmax) stochastic policy over the $Q$-values is used:
$\pi_{\theta}(s, a) = \frac{e^{\beta Q_{\theta}(s,a)}}{\sum_{a' \in A}
  e^{\beta Q_{\theta}(s,a')}}$, where $Q_\theta(s, a)$ is the $Q$-function
when the reward function is parameterized by $\theta$.

\commenta{Bayesian IRL!}

\commenta{Nonparametric Bayesian IRL?}

\commenta{Receeding-horizon IRL}

IRL algorithms are typically computationally demanding because they
require planning in their inner loops.
%If the social reward function learning is going to be used in any
%moderately interactive setting, this limitation may be prohibitive. 
%
\mydef{Receding horizon IRL}
(RHIRL)~\cite{macglashan15b} addresses this limitation by replacing
the usual infinite-horizon policy with a receding horizon controller
(RHC) that only plans out to some finite horizon from any given state,
thereby bounding computation time. An RHC tends to steer an IRL
algorithm towards a reward function with shaping values: short-term
rewards that guide the agent in the right direction without having to
plan too far ahead. Since these are precisely the kinds of values we
want our bias functions to capture, RHIRL allows us to plan with a
short horizon, thereby saving on computation time, which enables
interactive learning, described next.

\commenta{orthogonal to the above, and hence can be used with ANY of the above!}
