
\vspace{\up}
\paragraph{Inverse reinforcement learning}

Although there are multiple IRL formalizations and approaches, they
all take as input an MDP together with a \mydef{family} of a reward
functions $R_\Theta$ that is defined by some parameter space $\Theta$,
and a dataset $D$ of trajectories (where a trajectory is a finite
sequence of state-action pairs: $\langle (s_1, a_1), ..., (s_n, a_n)
\rangle$). The algorithms then output a specific parameterized reward
function $R_\theta \in R_\Theta$, which induces a policy that is
consistent with the input trajectories.
%
Different IRL algorithms frame the objective function for policy
consistency differently. One common approach is to treat the
search problem as a probabilistic inference
problem~\cite{babes11,lopes2009active,ramachandran2007bayesian,ziebart2008maximum}. 

For example, in the maximum-likelihood setting~\cite{babes11}, the
goal is to find a reward function parameterization that maximizes
the likelihood of the data:
%
\begin{equation}
\label{eq:mlirl}
\theta \in \arg\max_{\theta} L(D \mid R_{\theta}) = \prod_{t \in D} \prod_i^{|t|} \pi_{\theta}(s_i, a_i),
\end{equation}

\noindent
where $\pi_{\theta}(s, a)$ is a stochastic policy defining the
probability of taking action $a$ in state $s$ when the parameterized
reward function to be maximized is $R_{\theta}$. Typically, the
Boltzmann (softmax) stochastic policy over the $Q$-values is used:
$\pi_{\theta}(s, a) = \frac{e^{\beta Q_{\theta}(s,a)}}{\sum_{a' \in A}
  e^{\beta Q_{\theta}(s,a')}}$, where $Q_\theta(s, a)$ is the $Q$-function
when the reward function is parameterized by $\theta$.
%
Popular methods for solving Equation~\ref{eq:mlirl} (i.e., for
carrying out maximum-likelihood IRL (MLIRL)) include gradient ascent
and expectation-maximization.
%\commenta{do these algorithms need references?}

In Bayesian IRL, the goal is to compute a posterior distribution over
reward function parameterizations:
%
%\begin{equation}
%\label{eq:birl}
$\Pr(R_{\theta} \mid D) \propto \Pr(D \mid R_{\theta}) \Pr(R_\theta)$.
%\end{equation}
%
Bayesian IRL algoriths typically rely on Markov chain Monte Carlo
methods~\cite{journals/corr/abs-1208-2112,ramachandran2007bayesian}.
%
Other approaches include maximum-entropy~\cite{ziebart2008maximum}
and relative entropy IRL~\cite{BoulariasKP2011}. 
%
More recently, work has also commenced on
nonparametric Bayesian IRL methods~\cite{Choi:2012:NBI:2999134.2999169,Michini2012}.

IRL algorithms are typically computationally demanding because they
require planning in their inner loops.
%If the social reward function learning is going to be used in any
%moderately interactive setting, this limitation may be prohibitive. 
%
Compatible with all of the above algorithms, \mydef{receding horizon
IRL} (RHIRL)~\cite{macglashan15b} addresses this limitation by
replacing the usual infinite-horizon policy with a receding horizon
controller (RHC) that only plans out to some finite horizon from any
given state, thereby bounding computation time. An RHC tends to steer
an IRL algorithm towards a \mydef{shaping reward function}~\cite{Ng:1999:PIU:645528.657613}
%https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf
short-term rewards that guide the agent without it having to plan too
far ahead.  Consequently, RHIRL allows us to plan with a short
horizon, thereby saving on computation time.

Our computational framework, and the generalized model of IRL we
introduce in Section~\ref{sec:girl}, are all agnostic about the
particular approach to IRL taken.  We plan to experiment with multiple
variations.


