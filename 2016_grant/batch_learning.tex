
\vspace{\up}
\paragraph{Batch Learning}
\label{sec:batch}

The batch-learning setting is similar to the standard IRL problem in
that the algorithm is given a batch of demonstrations and a family of
utility functions over which to search. In our case, however, the batch
data consists of trajectories of states and joint actions in a
stochastic game, and the utility function family is a social utility
function family.  To learn the parameterization of the social utility
function family that best captures the behavior exhibited in the
demonstrations, we convert the stochastic game into an MDP,
%(in which joint actions correspond to actions, etc.),
and then use IRL to learn a parameterization of the utility function
family in this MDP.

\begin{algorithm}[t]
\caption{Batch\_Learning($(I,S,A^I,T,R^I), \gamma, D, {\mathcal T}, B_\Theta$)}
\label{alg:srl}
\begin{algorithmic}
\Require stochastic game $(I,S,A^I,T,R^I)$; discount factor $\gamma$; multi-agent demonstrations $D$; team function ${\mathcal T}$; and parameterized bias function family $B_\Theta$.
\State $A^M := \times_i A^I$ \Comment{Joint action set}
\State $R^M(s, a, s') := {\mathcal T}(R^I, s, a, s')$ \Comment{Team function}
% zzz the word reward was in there, but I don't think that's something we defined.
\State $R^S_\Theta(s, a, s') := R^M(s, a, s') + B_\Theta(s, a, s')$ \Comment{Social utility function family}
\State $R^S_\theta :=$ IRL($(S,A^M,T,R^S_\Theta), \gamma, D$)
\State \Return $R^S_\theta$ \Comment{Learned social utility function}
\end{algorithmic}
\end{algorithm}

%The batch learning algorithm is shown in Algorithm~\ref{alg:srl}
Pseudocode for the batch-learning algorithm is shown in Algorithm~\ref{alg:srl}.
%
The algorithm takes as input a stochastic game, a discount factor, a
set of multi-agent demonstrations (which perhaps adheres to some
equilibrium: i.e., collaborative behavior), a team function, and a
family of bias functions defined by some parameter space $\Theta$.
%
The first step of the batch-learning algorithm is to transform the
stochastic game into an MDP, and the second to apply IRL to that MDP.
%% %
%% Since the demonstrations are a set of trajectories of state-joint
%% action pairs, specifying the behavior of all the agents in the
%% environment, the first step of the batch learning algorithm is to
%% transform the stochastic game into an MDP. For the most part, this
%% transformation is straightforward: the states are the same, the MDP
%% action set is the space of joint actions, and the MDP transition
%% dynamics still operate on joint actions (which is the action set in
%% the MDP). A family of social reward functions is then defined as a
%% linear combination of the input team function (which depends on the
%% stochastic game's reward functions), and the family of bias
%% functions.\footnote{A parameter controlling the linear
%% combination of the team and bias function may be optimized as well.}
%% %
After learning a social utility function, any single agent can then
behave by selecting a joint action from the policy that results from
it, and then selecting their individual action from the joint. That
is, agent $i$ takes action $j^i \in A^i$, where 
$j \in \arg \max_j Q_{R^S_\theta}(s, j)$.

%Additionally, we set the zero-horizon value of states in RHIRL to the team value function. Setting the zero-horizon state values to the team value function allows RHIRL to use a short horizon a with bias function family that uses all of it representation power to express behavior biases rather than the ultimate task goals. To demonstrate that property, consider the fact that the RHC with a horizon of one and a bias function that always returns zero, always returns the original team value function when its horizon-zero state values are set to the team value function. The same is true for any horizon $h \ge 0$. As a result, the bias function only needs to focus its representation power on values that motivate collaborative behavior and the horizon used only needs to be large enough to motivate behavior that can exploit the bias values. Since computing the team value function only has to performed once and since we expect preferences for behavior to only require small horizons to exploit in practice, we expect the social reward function learning algorithm to be very computationally efficient.

