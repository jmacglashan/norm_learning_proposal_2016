
\subsubsection*{\large Selecting a Selfish or Social Stance}
\label{sec:stance}

We expect the artificial agents we build to be able to interact with
one another, and with humans, and infer social utilities that will
guide the collective towards mutually beneficial behavior.  In
contrast to existing machine-learning approaches that achieve
cooperative behavior by making strong assumptions about the algorithms
other agents in the environment adopt (e.g.,~\cite{conitzer07}), our
approach to coordinatation relies only on a shared history among
agents, at a purely behavioral (i.e., observational) level.

Nevertheless, we have for some time now been making a key assumption
about the intentions of other agents, namely that they, too, have the
goal of adopting joint goals.  In our initial experiments among humans
(only) in the Hallway game, we observed that some pairs readily
cooperated, while others adopted a competitive or selfish stance
toward the interaction.  In the most interesting cases, one player
began with a selfish stance, trying to block the other agent by
getting to the goal first, while the other adopted a social stance,
creating opportunities for both players to succeed.  There are several
important computational problems that arise when the two agents adopt
conflicting stances.  We briefly list these problems here, and a
proposed approach to each:

\begin{enumerate}

\item \emph{How can an agent detect that its stance differs from another's?}
  In preliminary work~\cite{kleiman16}, we adopted a Bayesian
  perspective to classify the other agents in one's environment as
  selfish or social (competitive or cooperative, in the terminology of
  that earlier project) from observed trajectories. We will evaluate a
  selfish/social classifier on the data we collect from human--human
  interactions to see if its judgments match those of human
  observers. We will then deploy the classifier in human--machine
  interactions to see whether it can be used to behave socially
  without taking on too much risk of exploitation.
  
\item \emph{If an agent adopts a social stance while another agent in 
  its environment adopts a selfish stance, how can the social agent
  convince the selfish agent to change its behavior?}  Our analysis of
  CD strategies (see Section~\ref{sec:grid_games}) provides a partial
  answer---an agent using such a strategy encourages another, even if
  it is inherently selfish, to behave in a mutually beneficial way.
  In particular, a CD strategy allows for cooperation but prevents
  exploitation, by making the other player an offer it would be
  foolish to refuse.  If a CD strategy does not exist, an agent could
  use a strategy that, across repeated games, provides the same
  benefits as CD strategies within games~\cite{munoz08}. It is
  challenging for an agent to capture the attention of another to
  trigger behavior change, so we expect interesting problems to be
  revealed.

\item \emph{When should an agent stop trying to pursuade others to
  adopt a social stance?}  There is a cost to remaining social while
  other agents are behaving selfishly.  The decision of when to
  conclude that another agent is unlikely to be convinced to change
  its behavior can be formulated as a reinforcement-learning problem
  in its own right~\cite{boutilier1996planning}.  We will investigate
  this problem, bringing to bear techniques from the study of
  partially observable Markov decision process~\cite{kaelbling98}, so
  that agents can reason effectively about the unobservable stances of
  others, as well as whether it can influence them, and if so, how.
  
\end{enumerate}

% Evaluation of these ideas is here !
Based on preliminary experiments with human--machine interactons, we
conjecture that stance selection is an essential capability for
socially rational agents.  Consequently, we are proposing to integrate
this decision into our computational framework (Step~1).  We will
evaluate our agents in a controlled environment, with and without this
capability, to assess how effective it is in fostering collaborative
interactions.
