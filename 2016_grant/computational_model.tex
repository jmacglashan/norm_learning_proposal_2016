
\section{Computational Framework}
\label{sec:process}

Recall, from them Introduction, our computational model:
1.~agents represent social preferences using a social utility function;
2.~based on their estimate of this function, agents use team reasoning to make plans to act;
3.~agents act, simultaneously or sequentially, as is appropriate in their environment;
4.~agents update their estimates of their social utility functions.

While certain aspects of social preferences (e.g., fairness) are broadly applicable,
Step~1 generally requires a representation that is tailored to the specific environment and actors within.
Likewise, Step~3 involves a straightforward simulation in grid games, but is
otherwise specific to the robots and their environment.

Only Steps~2 and~4 are sufficiently generic to apply across
environments.  Step~2 is a \mydef{planning} step; as planning is
generally considered a solved problem,\commenta{Michael, is this
  true?!} it can be accomplished using any off-the-shelf planner for
stochastic sequential decision making
environments~\cite{Barto95,bellman57,boutilier99,collins95,kearns99b,kocsis06}.
%
Step~4, on the other hand, is not a solved problem; indeed, in this
proposal,
%see Section~\ref{sec:feedback}
we describe an innovative approach to this problem.

Step~4 can be thought of as a \mydef{learning from demonstration}
problem~\cite{argall09}, in which an agent learns how to behave in an
environment by observing an expert.  That is, an agent is presented
with a data set consisting of multiple examples of behaviors (e.g.,
state--action trajectories through a Markov decision process), and is
tasked with the objective of learning a policy capable of (closely)
reproducing the data.

Two widely studied approaches to this problem include
\mydef{imitation} and \mydef{intention} learning.  Imitation learning
is a supervised learning approach in which an agent learns a mapping
from states to actions, with the expert's behavior serving as training
data~\cite{pomerleau93}.  After training, a learner follows its
learned policy directly.

Learning intentions~\cite{macglashan15b}, in contrast, is often framed
as an \mydef{inverse reinforcement learning} (IRL)
problem~\cite{babes11,ng00}.  Here, the agent's goal is to learn
rewards that motivate the expert's behavior, after which it applies a
planning algorithm to derive a policy that is consistent with those
learned rewards.  A strength of intention learning is that even simple
reward functions can capture complex behavior, leading to greater
generalization capabilities (i.e., appropriate behavior in
as-yet-unforeseen states) than the more direct approach taken by
imitation learning.

We are proposing to use IRL in Step~4.  Preliminary experiments,
described herein, with one off-the-shelf IRL algorithm, demonstrate
the potential of our computational model to facilitate collaborative
behavior among artificial agents.
%
But to better support our goal of human--machine collaboration, we
also propose a new approach to IRL in which demonstrations are not, by
default, assumed to have been generated by an expert.
%
With this additional power, our IRL approach can be applied to all
past interactions among agents, separating the successful from the
unsuccessful.
%
%Moreover, it need not apply only to complete runs; rather, sub-runs
%(i.e., options~\cite{sutton99}) can also be identified as successful
%and/or unsuccessful.
%
Indeed, in the computational process we put forth, learning is online,
not off, so not all demonstrations should be interpreted as
successful.  On the other hand, as past interactions are shared
context across agents, ideally they could \emph{all\/} provide crucial
guidance for identifying cooperative behavior, and that is what we set
out to achieve.

