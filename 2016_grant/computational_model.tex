
\section{Computational Framework}
\label{sec:process}

Recall, from the introduction, our computational model:
0.~agents represent social preferences using a social utility function;
1.~agents decide whether selfish or social behavior is appropriate;
2.~agents plan accordingly, using best-reply or team reasoning;
3.~agents act, simultaneously or sequentially, as dictated by their environment;
4.~agents update their estimates of their social utility functions.

While certain aspects of social preferences (e.g., fairness) are
broadly applicable, Step~0 requires a representation that is tailored
to the specific environment and actors within.  Likewise, Step~3
involves a straightforward simulation in grid games, but is otherwise
specific to the robots and their environment.

Only Steps~1, 2 and~4 are sufficiently generic to apply across
environments.  Step~2 is a \mydef{planning} step; as planning is a
well-defined optimization problem, it can be accomplished using any
off-the-shelf planner for stochastic sequential decision making
environments~\cite{Barto95,bellman57,boutilier99,collins95,kearns99b,kocsis06}.
%
Steps~1 and~4, on the other hand, are not solved problems; indeed, in
this proposal, we describe innovative approaches to these problems.

Step~4 can be thought of as a \mydef{learning from demonstration}
problem~\cite{argall09}, in which an agent learns how to behave in an
environment by observing an expert.  That is, an agent is presented
with a data set consisting of multiple examples of behaviors (e.g.,
state--action trajectories through a Markov decision process), and is
tasked with the objective of learning a policy capable of (closely)
reproducing the data.

Two widely studied approaches to this problem include
\mydef{imitation} and \mydef{intention} learning.  The former is a
supervised learning approach in which an agent learns a mapping from
states to actions, with the expert's behavior serving as training
data~\cite{pomerleau93}.  After training, a learner follows its
learned policy directly.

Learning intentions~\cite{macglashan15b}, in contrast, is often framed
as an \mydef{inverse reinforcement learning} (IRL)
problem~\cite{babes11,ng00}.  Here, the agent's goal is to learn
utilities that motivate the expert's behavior, after which it applies a
planning algorithm to derive a policy that is consistent with those
inferred utilities.  A strength of intention learning is that even simple
utility functions can capture complex behavior, leading to greater
generalization capabilities (i.e., appropriate behavior in
as-yet-unforeseen states) than the more direct approach taken by
imitation learning.

We are proposing to use IRL in Step~4.  Preliminary experiments,
described herein, with one off-the-shelf IRL algorithm, demonstrate
the potential of our computational model to facilitate collaborative
behavior among artificial agents.
%
But, to better support our goal of human--machine collaboration, we
also propose a new approach to IRL in which demonstrations are not, by
default, assumed to have been generated by an expert.
%
With this additional power, our IRL approach can learn from all
past interactions among agents, given a way of noting the successful and
unsuccessful interactions.
%
%Moreover, it need not apply only to complete runs; rather, sub-runs
%(i.e., options~\cite{sutton99}) can also be identified as successful
%and/or unsuccessful.
%
Indeed, in the computational process we put forth, learning is online,
not off, so not all demonstrations should be interpreted as
successful.  On the other hand, as past interactions are shared
context across agents, ideally they would \emph{all\/} provide crucial
guidance for identifying collaborative behavior, and that is what we
set out to achieve.

\comment{
WHY INTENTION LEARNING BEATS SUPERVISED METHODS!

Recall our working definition of a social norm as a behavioral
instruction that members of the community expect one another to
follow.  This definition suggests that norms could potentially be
learned in a supervised fashion simply by learning a function that
maps states to joint actions directly, rather than capturing norms in
a social utility function that motivates joint actions.
%\commenta{do you mean just hard code a norm in the bias functions? 
%does something like this next sentence capture what you mean?}
%\commenta{This definition suggests that norms could be directly encoded in the bias functions of a social utility function as bonuses associated with specific joint actions.}
However, as an environment and a target policy become more complex,
directly learning the policy becomes more challenging, and
generalizing from it likely less successful.  As with standard IRL,
the advantage of learning a social utility function instead is that
simple utility functions can often induce complex
behaviors \emph{across states}.  For example, consider a seemingly
straightforward norm that two agents approaching one another each stay
to their right.  If the action space is over low-level controls (e.g.,
rotations and small movements) and the policy is context dependent
(e.g., navigating obstacles to get to the right side), implementing
all the joint action rules necessary to make the agents pass on the
right could be difficult to specify (and hence, learn).  However, a
social utility function can simply define a bonus for when the agents
move past each other on the right and the planning algorithm does the
rest.
}

