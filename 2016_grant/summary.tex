
\centerline{\Large \bf Socially Rational Artificial Agents:}

\vspace{\down}
\centerline{\large \bf A Key to Human-Machine Collaboration}

\vspace{\up}
\paragraph{Overview}

\emph{Under the assumption that human agents are, perhaps boundedly
  but nonetheless ideally, socially rational creatures, we propose to
  design and build socially rational artificial agents that learn via
  repeated interactions, with the aim being for such agents to learn
  to collaborate effectively with humans.}

\comment{
\emph{We propose to develop collaborative learning algorithms for 
artificial agents in an effort to foster human-machine
collaborations.}

\emph{We propose an approach to the design of artificial agents,
by which they interact with humans, simultaneously learning social
preferences and collaborative behaviors that reinforce one another.}
}

%The ultimate goal of this project is to design artificial agents that
%play well with humans.  We propose to achieve this goal by building
%agents that infer peoples' utility functions, and then learn to
%collaborate appropriately based on these inferences.

{\bf Keywords}: reinforcement learning, stochastic games, behavioral experiments, social preferences.

\vspace{\up}
\paragraph{Intellectual Merit}

Much of human social life occurs in contexts where people must
coordinate their actions with those of others.  From party planning to
space exploration to grant-proposal evaluation, groups of people have
accomplished great things by reasoning as a team and engaging in
jointly intentional behavior.  Indeed, some have argued that, because
most other animals lack the capacity to work adaptively as cohesive
unit across many domains, team reasoning may be the hallmark of human
sociality.

We call optimal decision making, when agents hold social preferences
and employ team reasoning, socially rational behavior.  Socially
rational agents optimize a social utility function (i.e., a
representation of social preferences), which is sufficiently rich to
incorporate perceived (possibly inferred, possibly imagined) societal
benefits.  We assume that social utilities can be broken down into two
components---an objective component, which is usually a direct
function of the rules of interaction, and a subjective component,
which captures notions of distributivity and reciprocity.

Given these assumptions, we propose an iterative computational model
in which socially rational artificial agents construct social
preferences via repeated interactions with other, potentially human,
agents.  We contend that socially rational behavior, in which agents
jointly optimize a learned social utility function that reflects
constructed social preferences, is a promising new avenue for
orchestrating effective collaborations between humans and machines.

Central to our computational model is the notion of inverse
reinforcement-learning (IRL), whereby an agent is shown demonstrations
of behavior, based on which it infers utilities that motivate that
behavior.  Nearly all existing IRL algorithms to date assume the
demonstrations are generated by an expert.  In our setting, in
contrast, the demonstrations will be past interactions among agents
who are not necessarily skilled at the task at hand, but, on the
contrary, (hopefully) learning to collaborate.  Consequently, we are
proposing to develop new IRL technology that learns from
%(which is presently in its infancy)
both bad and good examples of behavior.  This technology will enable
humans to give agents both positive and negative feedback while they
are learning to collaborate.


%We will also develop and evaluate a new inverse reinforcement-learning
%algorithm that can construct social utility functions by analyzing
%the successes and failures of past interactions.


\vspace{\up}
\paragraph{Broader Impact}
%A wide variety of human--machine interaction problems would be
%positively impacted by the technology we are proposing.
%
This project is synergistic with Brown University's Humanity Centered
Robotics Initiative (HCRI), of which co-PI Littman is co-director.
Specifically, within HCRI, there are ongoing efforts to design robotic
systems that interact with people and support independent living tasks
(e.g., gerontechnological support for aging in place).  For an elderly
person to trust and collaborate on tasks with a machine effectively,
the machine must act in a manner that the elderly person expects.
This project will create the foundation for these important
applications.

We will also create a new undergraduate course called ``Social
autonomous driving.''  It will be offered as part of a new robotics
course sequence being developed at Brown.  In our course, students
will develop robots that drive around a test environment.  The main
emphasis will be on making sure the robotic drivers interact smoothly
with other robotic and remote-controlled cars.

We also plan to integrate our work on this project into Artemis, a
free summer program that introduces rising 9th grade girls to
computational thinking.  For example, we might have the Artemis girls
teach a robot to collaborate with them on routine tasks, such as
navigation or object search.

Our deliverables include an open-source publicly accessible toolkit
for implementing human-machine collaborative-learning tasks via
reinforcement learning. Further, we will maintain a database of
machine--machine, human--machine, and human--human experimental results
built using this toolkit, which can serve as a benchmark for future
researchers who also seek to build artificial agents that increasingly
achieve human-like behavior.
%
Finally, we expect to publish the results of the proposed research in
top-tier archival conference proceedings and journals with high
impact factors, and to present our work at innovative, non-archival
workshops (e.g., the AAAI symposia).

