
\section{Evaluation}

We will evaluate our approach in multiple ways. First, we will test
our interactive learning algorithm with other human players in our
experimental testbed and compare how often and quickly humans are able
to arrive at coordinated behavior with our artificial agent versus
other humans. To test the expressiveness of our algorithm's space of
learnable solutions, we will also evaluate how many different kinds of
strategies can be found when players play with our agent and compare
that to the set of strategies that can be found by humans playing with
each other.  Subjective evaluations from the human participants will
be used to assess whether interacting with the agent feels natural.

For the batch learning algorithm, we will develop new quantitative
metrics to assess behavior similarity so that trajectories produced
from the batch learning can be quantitatively compared to the actual
human participants. Additionally, using our experimental testbed, we
will have two humans interact for a number of sessions and then,
without notification to the users, swap out their partner with an
artificial agent trained from the history of interactions they had
with their human partner. At the end of all interactions, we will ask
the user whether they thought they were playing with the same partner
the whole time or if their partner switched. Their subjective response
to whether they thought their partner was switched at any point will
be compared to their subjective response when their partner is swapped
out with an artificial agent that did not perform any learning and
when their partner is swapped with a different human player.

Finally, we are also interested in cooperative behavior generalizing
to new state spaces. Using our environmental testbed, we will have
participants play in a sequence of different grids and using the same
kinds of evaluations listed above, test whether our algorithm can
safely generalize to new grids as well as other humans can.

% Evaluation metrics that capture trajectory similarity across diverse
% games are key for assessing performance, and will drive the
% development of our learning algorithms towards more beneficial
% behavior.
% 
% We will also develop ways for people to also assess how natural it is
% to interact with our artificial agents. For example, we will
% investigate whether having two humans interact for awhile, and then,
% without notifying them, swapping their partner with an artificial
% agent trained from the interaction history results in a noticeable
% change in behavior. Participants would be asked if they noticed a
% change, and behavior would be compared between a variety of cases, 
% such as: no swap; or swapping in a trained agent, an untrained agent, 
% a human who witnessed the history of interactions, a human who did not.
% 
% Throughout the project, we will investigate how to design social
% utility functions that generalize across environments. We we will
% start by examining standard feature selection methods used in existing
% reinforcement-learning and IRL research (e.g., linear models, neural
% networks,
% etc.)~\cite{diuk2009adaptive,kolter2009regularization,li2009reinforcement,parr2008analysis},
% and build on them, as necessary.
% % ML: Note that we could propose ``deep IRL''... since MLIRL is a
% % gradient method, it should be easy to incorporate it into standard
% % deep learning packages to learn very complex social utilities.
% We will evaluate generalization by first having users learn in one
% grid. Afterwards, they will be exposed to a new grid where the
% previous behavior is no longer directly applicable (e.g., because a
% wall was inserted along the path that an agent would normally take),
% but a more ``general'' behavior can be transfered (e.g., I take the
% high road; he takes the low, regardless of the presence or absence of
% obstacles).
